{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 및 자연어 빅데이터 분석방법론/  컴퓨터언어학연구 I\n",
    "\n",
    "## Confusion Matrix for Your Multi-Class Machine Learning Model\n",
    "\n",
    "- based on https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a tabular way of visualizing the performance of your prediction model. Each entry in a confusion matrix denotes the number of predictions that were made by the model where it classified the classes correctly or incorrectly.\n",
    "Anyone who is already familiar with the confusion matrix knows that most of the time it is explained for a binary classification problem. Well, this explanation is not one of them. Today we will see how does a confusion matrix work on multi-class machine learning models. However, we will start with a little background using a binary classification just to put things in perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Binary Classification\n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*fxiTNIgOyvAombPJx5KGeA.png)\n",
    "\n",
    "\n",
    "As you can see, a binary classification problem has only two classes to classify, preferably a positive and a negative class. Now let’s look at the metrics of the Confusion Matrix.\n",
    "\n",
    "- **True Positive (TP)**: It refers to the number of predictions where the classifier correctly predicts the positive class as positive.\n",
    "- **True Negative (TN)**: It refers to the number of predictions where the classifier correctly predicts the negative class as negative.\n",
    "- **False Positive (FP)**: It refers to the number of predictions where the classifier incorrectly predicts the negative class as positive.\n",
    "- **False Negative (FN)**: It refers to the number of predictions where the classifier incorrectly predicts the positive class as negative.\n",
    "\n",
    "It’s always better to use confusion matrix as your evaluation criteria for your machine learning model. It gives you a very simple, yet efficient performance measures for your model. Here are some of the most common performance measures you can use from the confusion matrix.\n",
    "\n",
    "- **Accuracy**: It gives you the overall accuracy of the model, meaning the fraction of the total samples that were correctly classified by the classifier. To calculate accuracy, use the following formula: **(TP+TN)/(TP+TN+FP+FN)**.\n",
    "\n",
    "- **Misclassification Rate**: It tells you what fraction of predictions were incorrect. It is also known as Classification Error. You can calculate it using **(FP+FN)/(TP+TN+FP+FN) or (1-Accuracy)**.\n",
    "\n",
    "- **Precision**: It tells you what fraction of predictions as a positive class were actually positive. To calculate precision, use the following formula: **TP/(TP+FP**).\n",
    "\n",
    "- **Recall**: It tells you what fraction of all positive samples were correctly predicted as positive by the classifier. It is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. To calculate Recall, use the following formula: **TP/(TP+FN)**.\n",
    "\n",
    "- **Specificity**: It tells you what fraction of all negative samples are correctly predicted as negative by the classifier. It is also known as True Negative Rate (TNR). To calculate specificity, use the following formula: **TN/(TN+FP)**.\n",
    "\n",
    "- **F1-score**: It combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall. It can be calculated as follows:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*wUdjcIb9J9Bq6f2GvX1jSA.png)\n",
    "\n",
    "Now, in a perfect world, we’d want a model that has a precision of 1 and a recall of 1. That means a F1-score of 1, i.e. a 100% accuracy which is often not the case for a machine learning model. So what we should try, is to get a higher precision with a higher recall value. Okay, now that we know about the performance measures for confusion matrix, Let’s see how we can use that in a multi-class machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Multi-Class Classification\n",
    "\n",
    "For simplicity’s sake, let’s consider our multi-class classification problem to be a 3-class classification problem. Say, we have a dataset that has three class labels, namely Apple, Orange and Mango. The following is a possible confusion matrix for these classes.\n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*yH2SM0DIUQlEiveK42NnBg.png)\n",
    "\n",
    "Unlike binary classification, there are no positive or negative classes here. At first, it might be a little difficult to find TP, TN, FP and FN since there are no positive or negative classes, but it’s actually pretty easy. What we have to do here is to find TP, TN, FP and FN for each individual class. For example, if we take class Apple, then let’s see what are the values of the metrics from the confusion matrix.\n",
    "\n",
    "- TP = 7\n",
    "- TN = (2+3+2+1) = 8\n",
    "- FP = (8+9) = 17\n",
    "- FN = (1+3) = 4\n",
    "\n",
    "Since we have all the necessary metrics for class Apple from the confusion matrix, now we can calculate the performance measures for class Apple. For example, class Apple has\n",
    "\n",
    "- Precision = 7/(7+17) = 0.29\n",
    "- Recall = 7/(7+4) = 0.64\n",
    "- F1-score = 0.40\n",
    "\n",
    "Similarly, we can calculate the measures for the other classes. Here is a table that shows the values of each measure for each class.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*X1ghULso7P3AdMalomM6yg.png)\n",
    "precision, REcall and F1-score for Each Class\n",
    "\n",
    "Now we can do more with these measures. We can combine the F1-score of each class to have a single measure for the whole model. There are a few ways to do that, let’s look at them now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro F1\n",
    "\n",
    "This is called micro-averaged F1-score. It is calculated by considering the total TP, total FP and total FN of the model. It does not consider each class individually, It calculates the metrics globally. So for our example,\n",
    "\n",
    "- Total TP = (7+2+1) = 10\n",
    "- Toatal FP = (8+9)+(1+3)+(3+2) = 26\n",
    "- Total FN = (1+3)+(8+2)+(9+3) = 26\n",
    "\n",
    "\n",
    "Hence,\n",
    "\n",
    "- Precision = 10/(10+26) = 0.28\n",
    "- Recall = 10/(10+26) = 0.28\n",
    "\n",
    "Now we can use the regular formula for F1-score and get the Micro F1-score using the above precision and recall.\n",
    "\n",
    "**Micro F1 = 0.28**\n",
    "\n",
    "As you can see When we are calculating the metrics globally all the measures become equal. Also if you calculate accuracy you will see that,\n",
    "\n",
    "*Precision = Recall = Micro F1 = Accuracy*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro F1\n",
    "\n",
    "This is macro-averaged F1-score. It calculates metrics for each class individually and then takes unweighted mean of the measures. As we have seen from figure **“Precision, Recall and F1-score for Each Class”**,\n",
    "\n",
    "- Class Apple F1-score = 0.40\n",
    "- Class Orange F1-score = 0.22\n",
    "- Class Mango F1-score = 0.11\n",
    "\n",
    "Hence,\n",
    "\n",
    "**Macro F1 = (0.40+0.22+0.11)/3 = 0.24**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted F1\n",
    "\n",
    "The last one is weighted-averaged F1-score. Unlike Macro F1, it takes a weighted mean of the measures. The weights for each class are the total number of samples of that class. Since we had 11 Apples, 12 Oranges and 13 Mangoes,\n",
    "\n",
    "**Weighted F1 = ((0.40x11)+(0.22x12)+(0.11x13))/(11+12+13) = 0.24**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s look at a script to calculate these measures using Python’s Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 1 15  0]\n",
      " [ 0 21  0]\n",
      " [ 0  7  1]]\n",
      "\n",
      "Accuracy: 0.51\n",
      "\n",
      "Micro Precision: 0.51\n",
      "Micro Recall: 0.51\n",
      "Micro F1-score: 0.51\n",
      "\n",
      "Macro Precision: 0.83\n",
      "Macro Recall: 0.40\n",
      "Macro F1-score: 0.33\n",
      "\n",
      "Weighted Precision: 0.76\n",
      "Weighted Recall: 0.51\n",
      "Weighted F1-score: 0.39\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 1       1.00      0.06      0.12        16\n",
      "     Class 2       0.49      1.00      0.66        21\n",
      "     Class 3       1.00      0.12      0.22         8\n",
      "\n",
      "    accuracy                           0.51        45\n",
      "   macro avg       0.83      0.40      0.33        45\n",
      "weighted avg       0.76      0.51      0.39        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#importing a 3-class dataset from sklearn's toy dataset\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(kernel='rbf', C=1).fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
