{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAkF_BllJRMP"
   },
   "source": [
    "# Natural Language Is Fun\n",
    "- based on https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\n",
    "- based on SLP 3rd Edition.Draft by Jurafsky and Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yd67AdSCJRMR"
   },
   "source": [
    "## Natural Language Processing/Computational Linguistics?\n",
    "- The study of human languages and how they can be represented computationally and analyzed and generated algorithmically\n",
    "    - The cat is on the mat. --> on (mat, cat)\n",
    "    \n",
    "\n",
    "    - on (mat, cat) --> The cat is on the mat\n",
    "\n",
    "- Studying NLP involves studying natural language, formal representations, and algorithms for their manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7Mj7AAXJRMV"
   },
   "source": [
    "# Let's Get Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8hzY_PnJRMV"
   },
   "source": [
    "## Understanding Raw Text\n",
    "![](https://miro.medium.com/max/2000/1*CtR2lIHDkhB9M8Jt4irSyg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ey3dMhGJRMW"
   },
   "source": [
    "- *Natural Language Processing*, or NLP, is the sub-field of AI that is focused on enabling computers to understand and process human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UbefDg1JRMW"
   },
   "source": [
    "## Can Computers Understand Language?\n",
    "- As long as computers have been around, programmers have been trying to write programs that understand languages like English. The reason is pretty obvious — humans have been writing things down for thousands of years and it would be really helpful if a computer could read and understand all that data.\n",
    "- Computers can’t yet truly understand English in the way that humans do — but they can already do a lot! In certain limited areas, what you can do with NLP already seems like magic. You might be able to save a lot of time by applying NLP techniques to your own projects.\n",
    "- And even better, the latest advances in NLP are easily accessible through open source Python libraries like spaCy, textacy, and neuralcoref. What you can do with just a few lines of python is amazing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bDFdtYzJRMW"
   },
   "source": [
    "## Extracting Meaning from Text is Hard\n",
    "- The process of reading and understanding English is very complex — and that’s not even considering that English doesn’t follow logical and consistent rules. For example, what does this news headline mean?\n",
    "    - *“Environmental regulators grill business owner over illegal coal fires.”\n",
    "- Are the regulators questioning a business owner about burning coal illegally? Or are the regulators literally cooking the business owner? As you can see, parsing English with a computer is going to be complicated.\n",
    "- Doing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things.\n",
    "- And that’s exactly the strategy we are going to use for NLP. We’ll break down the process of understanding English into small chunks and see how each one works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_oUeLujJRMX"
   },
   "source": [
    "## Building an NLP Pipeline, Step-by-Step\n",
    "- Let’s look at a piece of text from Wikipedia:\n",
    "   - *London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium.*\n",
    "\n",
    "- This paragraph contains several useful facts. It would be great if a computer could read this text and understand that *London is a city, London is located in England, London was settled by Romans and so on*. But to get there, we have to first teach our computer the most basic concepts of written language and then move up from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28wP2B3UJRMX"
   },
   "source": [
    "### Step 1: Sentence Segmentation\n",
    "- The first step in the pipeline is to break the text apart into separate sentences. That gives us this:\n",
    "    - *“London is the capital and most populous city of England and the United Kingdom.”\n",
    "    - *“Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia.”*\n",
    "    - *“It was founded by the Romans, who named it Londinium.”*\n",
    "- We can assume that each sentence in English is a separate thought or idea. It will be a lot easier to write a program to understand a single sentence than to understand a whole paragraph.\n",
    "- Coding a Sentence Segmentation model can be as simple as splitting apart sentences whenever you see a punctuation mark. But modern NLP pipelines often use more complex techniques that work even when a document isn’t formatted cleanly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlBTufCPJRMX"
   },
   "source": [
    "### Step 2: Word Tokenization\n",
    "- Now that we’ve split our document into sentences, we can process them one at a time. Let’s start with the first sentence from our document:\n",
    "    - *“London is the capital and most populous city of England and the United Kingdom.”*\n",
    "- The next step in our pipeline is to break this sentence into separate words or tokens. This is called tokenization. This is the result:\n",
    "    - *“London”, “is”, “ the”, “capital”, “and”, “most”, “populous”, “city”, “of”, “England”, “and”, “the”, “United”, “Kingdom”, “.”*\n",
    "- Tokenization is easy to do in English. We’ll just split apart words whenever there’s a space between them. And we’ll also treat punctuation marks as separate tokens since punctuation also has meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN4QyucIJRMX"
   },
   "source": [
    "### Step 3: Predicting Parts of Speech for Each Token\n",
    "- Next, we’ll look at each token and try to guess its part of speech — whether it is a noun, a verb, an adjective and so on. Knowing the role of each word in the sentence will help us start to figure out what the sentence is talking about.\n",
    "- We can do this by feeding each word (and some extra words around it for context) into a pre-trained part-of-speech classification model:\n",
    "![](https://miro.medium.com/max/2058/1*u7Z1B1TIYe68V8lS2f8GNg.png)\n",
    "- The part-of-speech model was originally trained by feeding it millions of English sentences with each word’s part of speech already tagged and having it learn to replicate that behavior.\n",
    "- Keep in mind that the model is completely based on statistics — it doesn’t actually understand what the words mean in the same way that humans do. It just knows how to guess a part of speech based on similar sentences and words it has seen before.\n",
    "- After processing the whole sentence, we’ll have a result like this:\n",
    "![](https://miro.medium.com/max/2436/1*O0gIbvPd-weZw4IGmA5ywQ.png)\n",
    "- With this information, we can already start to glean some very basic meaning. For example, we can see that the nouns in the sentence include “London” and “capital”, so the sentence is probably talking about London."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnPC6h68JRMY"
   },
   "source": [
    "### Step 4: Text Lemmatization\n",
    "- In English (and most languages), words appear in different forms. Look at these two sentences:\n",
    "    - *I had a pony.*\n",
    "    - *I had two ponies.*\n",
    "- Both sentences talk about the noun pony, but they are using different inflections. When working with text in a computer, it is helpful to know the base form of each word so that you know that both sentences are talking about the same concept. Otherwise the strings **“pony”** and **“ponies”** look like two totally different words to a computer.\n",
    "- In NLP, we call finding this process *lemmatization* — figuring out the most basic form or lemma of each word in the sentence.\n",
    "- The same thing applies to verbs. We can also lemmatize verbs by finding their root, unconjugated form. So **“I had two ponies”** becomes **“I [have] two [pony].”**\n",
    "- *Lemmatization* is typically done by having a look-up table of the lemma forms of words based on their part of speech and possibly having some custom rules to handle words that you’ve never seen before.\n",
    "- Here’s what our sentence looks like after lemmatization adds in the root form of our verb:\n",
    "![](https://miro.medium.com/max/2436/1*EgYJsyjBNk074TQf87_CqA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXmu8dTJRMZ"
   },
   "source": [
    "### Step 5: Identifying Stop Words\n",
    "- Next, we want to consider the importance of a each word in the sentence. English has a lot of filler words that appear very frequently like “and”, “the”, and “a”. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other words. Some NLP pipelines will flag them as stop words —that is, words that you might want to filter out before doing any statistical analysis.\n",
    "- Here’s how our sentence looks with the stop words grayed out:\n",
    "![](https://miro.medium.com/max/2436/1*Zgq1nK_71AzX1CaknB89Ww.png)\n",
    "- Stop words are usually identified by just by checking a hardcoded list of known stop words. But there’s no standard list of stop words that is appropriate for all applications. The list of words to ignore can vary depending on your application.\n",
    "- For example if you are building a rock band search engine, you want to make sure you don’t ignore the word *“The”*. Because not only does the word *“The”* appear in a lot of band names, there’s a famous 1980’s rock band called The The!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hL72hRfJRMZ"
   },
   "source": [
    "### Step 6: Dependency Parsing\n",
    "- The next step is to figure out how all the words in our sentence relate to each other. This is called dependency parsing.\n",
    "- The goal is to build a tree that assigns a single **parent** word to each word in the sentence. The root of the tree will be the main verb in the sentence. Here’s what the beginning of the parse tree will look like for our sentence:\n",
    "![](https://miro.medium.com/max/2574/1*nteaQRxNNSXMlAnT31iXjw.png)\n",
    "- But we can go one step further. In addition to identifying the parent word of each word, we can also predict the type of relationship that exists between those two words:\n",
    "![](https://miro.medium.com/max/2574/1*onc_4Mnq2L7cetMAowYAbA.png)\n",
    "\n",
    "- This parse tree shows us that the subject of the sentence is the noun *“London”* and it has a *“be”* relationship with “capital”.\n",
    "- We finally know something useful — London is a capital! And if we followed the complete parse tree for the sentence (beyond what is shown), we would even found out that London is the capital of the United Kingdom.\n",
    "- Just like how we predicted parts of speech earlier using a machine learning model, dependency parsing also works by feeding words into a machine learning model and outputting a result. \n",
    "- But parsing word dependencies is particularly complex task and would require an entire article to explain in any detail. If you are curious how it works, a great place to start reading is Matthew Honnibal’s excellent article *“Parsing English in 500 Lines of Python”*.\n",
    "- But despite a note from the author in 2015 saying that this approach is now standard, it’s actually out of date and not even used by the author anymore. \n",
    "- In 2016, Google released a new dependency parser called *Parsey McParseface* which outperformed previous benchmarks using a new deep learning approach which quickly spread throughout the industry. Then a year later, they released an even newer model called ParseySaurus which improved things further. In other words, parsing techniques are still an active area of research and constantly changing and improving.\n",
    "- It’s also important to remember that many English sentences are ambiguous and just really hard to parse. In those cases, the model will make a guess based on what parsed version of the sentence seems most likely but it’s not perfect and sometimes the model will be embarrassingly wrong. But over time our NLP models will continue to get better at parsing text in a sensible way.\n",
    "- Want to try out dependency parsing on your own sentence? There’s a great interactive demo from the spaCy team here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7wZpRH9JRMa"
   },
   "source": [
    "### Step 6b: Finding Noun Phrases\n",
    "- So far, we’ve treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing.\n",
    "- For example, instead of this:\n",
    "![](https://miro.medium.com/max/2436/1*EgYJsyjBNk074TQf87_CqA.png)\n",
    "<br>\n",
    "- We can group the noun phrases to generate this:\n",
    "\n",
    "![](https://miro.medium.com/max/1974/1*5dlHkuUP3pG8ktlR-wPliw.png)\n",
    "<br>\n",
    "<br>\n",
    "- Whether or not we do this step depends on our end goal. But it’s often a quick and easy way to simplify the sentence if we don’t need extra detail about which words are adjectives and instead care more about extracting complete ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyF2xIq4JRMa"
   },
   "source": [
    "### Step 7: Named Entity Recognition (NER)\n",
    "- Now that we’ve done all that hard work, we can finally move beyond grade-school grammar and start actually extracting ideas.\n",
    "<br>\n",
    "\n",
    "- In our sentence, we have the following nouns:\n",
    "\n",
    "![](https://miro.medium.com/max/2208/1*JMXGOrdx4oQsfZC5t-Ksgw.png)\n",
    "<br>\n",
    "- Some of these nouns present real things in the world. For example, *“London”*, *“England”* and *“United Kingdom”* represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP.\n",
    "- The goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here’s what our sentence looks like after running each token through our NER tagging model:\n",
    "\n",
    "![](https://miro.medium.com/max/2262/1*x1kwwACli8Fcvjos_6oS-A.png)\n",
    "\n",
    "- But NER systems aren’t just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between “Brooklyn Decker” the person and the place “Brooklyn” using context clues.\n",
    "- Here are just some of the kinds of objects that a typical NER system can tag:\n",
    "    - People’s names\n",
    "    - Company names\n",
    "    - Geographic locations (Both physical and political)\n",
    "    - Product names\n",
    "    - Dates and times\n",
    "    - Amounts of money\n",
    "    - Names of events\n",
    "\n",
    "- NER has tons of uses since it makes it so easy to grab structured data out of text. It’s one of the easiest ways to quickly get value out of an NLP pipeline.\n",
    "- Want to try out Named Entity Recognition yourself? There’s another great interactive demo from spaCy [here](https://explosion.ai/demos/displacy-ent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wN-oa6wsJRMb"
   },
   "source": [
    "### Step 8: Coreference Resolution\n",
    "- At this point, we already have a useful representation of our sentence. We know the parts of speech for each word, how the words relate to each other and which words are talking about named entities.\n",
    "- However, we still have one big problem. English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. \n",
    "- Humans can keep track of what these words represent based on context. But our NLP model doesn’t know what pronouns mean because it only examines one sentence at a time.\n",
    "<br>\n",
    "- Let’s look at the third sentence in our document:\n",
    "    - *“It was founded by the Romans, who named it Londinium.”*\n",
    "<br>\n",
    "- If we parse this with our NLP pipeline, we’ll know that *“it”* was founded by Romans. But it’s a lot more useful to know that *“London”* was founded by Romans.\n",
    "- As a human reading this sentence, you can easily figure out that *“it”* means *“London”*. The goal of coreference resolution is to figure out this same mapping by tracking pronouns across sentences. We want to figure out all the words that are referring to the same entity.\n",
    "- Here’s the result of running coreference resolution on our document for the word *“London”*:\n",
    "![](https://miro.medium.com/max/1874/1*vGPbWiJqQA65GlwcOYtbKQ.png)\n",
    "\n",
    "- With coreference information combined with the parse tree and named entity information, we should be able to extract a lot of information out of this document!\n",
    "<br>\n",
    "- Coreference resolution is one of the most difficult steps in our pipeline to implement. It’s even more difficult than sentence parsing. Recent advances in deep learning have resulted in new approaches that are more accurate, but it isn’t perfect yet. If you want to learn more about how it works, start here.\n",
    "<br>\n",
    "- Want to play with co-reference resolution? Check out this great co-reference resolution demo from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPAYqu8wJRMb"
   },
   "source": [
    "## Coding the NLP Pipeline in Python\n",
    "- Here’s an overview of our complete NLP pipeline:\n",
    "\n",
    "![](https://miro.medium.com/max/4304/1*zHLs87sp8R61ehUoXepWHA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvRoBv_hJRMc",
    "outputId": "052b21a5-cfa9-43f4-ad4a-e3ee09590801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6 MB 79 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.62.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting en-core-web-lg==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.1.0/en_core_web_lg-3.1.0-py3-none-any.whl (777.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 777.1 MB 12 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.1.0) (3.1.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (57.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (4.62.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-lg==3.1.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "Requirement already satisfied: textacy in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.19.5)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.8.8)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.62.0)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.4.1)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.2)\n",
      "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (3.1.2)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.1)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.22.2.post1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (8.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy>=3.0.0->textacy) (3.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.1.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy \n",
    "!pip3 install -U spacy\n",
    "\n",
    "# Download the large English model for spaCy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download en_core_web_lg\n",
    "\n",
    "# Install textacy which will also be useful\n",
    "!pip3 install -U textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zCfZh9byCQc9",
    "outputId": "e6ec247a-5cc3-488c-f5ef-b3736f7551db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lETdriD0CTyV",
    "outputId": "1aeac8ce-4c9e-4a8a-9813-7f4ba1aa86cb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textacy\n",
    "textacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0oc__sUJRMc"
   },
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkyzuyK0JRMd"
   },
   "source": [
    "- Then the code to run an NLP pipeline on a piece of text looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fA6UUzjVJRMd",
    "outputId": "dc5e440b-371c-4afd-b662-44c2f1674371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London (GPE)\n",
      "England (GPE)\n",
      "the United Kingdom (GPE)\n",
      "the south east (LOC)\n",
      "Great Britain (GPE)\n",
      "London (GPE)\n",
      "two (CARDINAL)\n",
      "Romans (NORP)\n",
      "Londinium (WORK_OF_ART)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two milennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the text with spaCy. This runs the entire pipeline.\n",
    "doc = nlp(text)\n",
    "\n",
    "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9P-mJ1YEJRMd",
    "outputId": "4da12d83-1da8-4d51-8c25-b17c9f10a31e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " In 1950 , [REDACTED]  published his famous article \" Computing Machinery and Intelligence \" . In 1957 , [REDACTED]  \n",
      " Syntactic Structures revolutionized [REDACTED]  with ' universal grammar ' , a rule based system of syntactic structures . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "\n",
    "# Load the large English NLP model\n",
    "#nlp = spacy.load('en_core_web_lg')\n",
    "# https://spacy.io/api/token#attributes for spacy token Attributes\n",
    "# Replace a token with \"REDACTED\" if it is a name\n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED] \"\n",
    "    else:\n",
    "        # return token.string\n",
    "        return token.text\n",
    "\n",
    "# Loop through all the entities in a document and check if they are names\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    # for ent in doc.ents:\n",
    "    #     ent.merge()\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            retokenizer.merge(ent)\n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
    "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
    "\"\"\"\n",
    "\n",
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tf5-Iz_JRMd"
   },
   "source": [
    "## Extracting Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08h2DKQ0JRMd",
    "outputId": "f5a5c442-53fb-4c2b-bed9-b8d32bbc5c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the things I know about London:\n",
      " - [the, capital, and, most, populous, city, of, England]\n",
      " - [a, major, settlement]\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "import textacy.extract\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "# Parse the document with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract semi-structured statements\n",
    "# statements = textacy.extract.semistructured_statements(doc, \"London\")\n",
    "statements = textacy.extract.semistructured_statements(doc, entity=\"London\", cue=\"be\")\n",
    "\n",
    "# Print the results\n",
    "print(\"Here are the things I know about London:\")\n",
    "\n",
    "for statement in statements:\n",
    "    subject, verb, fact = statement\n",
    "    print(f\" - {fact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3RdRxMlJRMe",
    "outputId": "fa4aec22-47b1-4516-8dc5-51f3d2bfb6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major settlement\n",
      "river thames\n",
      "two millennia\n",
      "south east\n",
      "united kingdom\n",
      "most populous city\n",
      "great britain\n"
     ]
    }
   ],
   "source": [
    "#import spacy\n",
    "#import textacy.extract\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The text we want to examine\n",
    "#text = \"\"\"London is [.. shortened for space ..]\"\"\"\n",
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    "# Parse the document with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract noun chunks that appear  Change min_freq=3 to 1\n",
    "noun_chunks = textacy.extract.noun_chunks(doc, min_freq=1)\n",
    "\n",
    "#print(noun_chunks)\n",
    "# Convert noun chunks to lowercase strings\n",
    "noun_chunks = map(str, noun_chunks)\n",
    "noun_chunks = map(str.lower, noun_chunks)\n",
    "#print(noun_chunks)\n",
    "# Print out any nouns that are at least 2 words long\n",
    "for noun_chunk in set(noun_chunks):\n",
    "    if len(noun_chunk.split(\" \")) > 1:\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QiVR3HPJRMe"
   },
   "source": [
    "# Text Classification is Your New Secret Weapon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAWX2Uq4JRMe"
   },
   "source": [
    "## Bottoms Up\n",
    "- The NLP pipeline that we set up in Part 1 processes text in a top down way. First we split text into sentences, then we break sentences down into nouns and verbs, then we figure out the relationships between those words, and so on. It’s a very logical approach and logic just feels right, but logic isn’t necessarily the best way to go about extracting data from text.\n",
    "- A lot of user-created content is messy, unstructured and, some might even say, nonsensical:\n",
    "![](https://miro.medium.com/max/2548/1*Ea0eGvOcNqo6DOrFcO5mSQ.png)\n",
    "- Extracting data from messy text by analyzing it’s grammatical structure is very challenging because the text doesn’t follow normal grammatical rules. We can get often get better results using dumber models that work from the **bottom up**. \n",
    "\n",
    "- Instead of analyzing sentence structure and grammar, we’ll just look for **statistical patterns** in word use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8P2loY9JRMe"
   },
   "source": [
    "## Using Classification Models to Extract Meaning\n",
    "- Let’s look at user reviews, one of the most common types of online data that you might want to parse with a computer. Here is one of my real Yelp reviews for a public park:\n",
    "![](https://miro.medium.com/max/2896/1*vOi7oFrm_gyKZyT4vEfpBw.png)\n",
    "- From the screenshot, you can see that I gave the park a 5-star review. But if I had posted this review without a star rating, you would still automatically understand that I liked the park from how I described it.\n",
    "- How can we write a program that can read this text and understand that I liked the park even though I never directly said “I like this park” in the text? The trick is to reframe this complex language understanding task as a simple **classification problem**.\n",
    "- Let’s set up a simple linear classifier that takes in words. The input to the classifier is the text of the review. The output is one of 5 fixed labels — “1 star”, “2 stars”, “3 stars”, “4 stars”, or “5 stars”.\n",
    "\n",
    "![](https://miro.medium.com/max/2058/1*hv4sHrjHK4J9zw27qB_r-g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpvoIY8RJRMe"
   },
   "source": [
    "\n",
    "- If the classifier was able to take in the text and reliably predict the correct label, that means it must somehow understand the text enough to extract the overall meaning of whether or not I liked the the park. \n",
    "- Of course the model’s level of “understanding” is just that it churns some data through a statistical model and gets a most likely answer. It’s not similar to human intelligence. But if the end result is the same most of the time, then it doesn’t really matter.\n",
    "- To train our text classification model, we’ll collect a lot of user reviews of similar places (parks, businesses, landmarks, hotels, whatever we can find…) where the user wrote a text review and assigned a similar star rating. And by lots, I mean millions of reviews! Then we’ll train the model to predict a star rating based on the corresponding text.\n",
    "- Once the model is trained, we can use it to make predictions for new text. Just pass in a new piece of text and get back a score:\n",
    "\n",
    "![](https://miro.medium.com/max/2058/1*bZfdLQtpEWo4CO-4r5w24g.png)\n",
    "\n",
    "- With this simplistic model, we can do all kinds of useful things. For example, we could start a company that analyzes social media trends. Companies would hire us to track how their brand is perceived online and to alert them of negative trends in perception.\n",
    "- To build that, we’d just scan for any tweets that mentioned our customer’s business. Then we’d feed all those tweets into the text classification model to predict if each user likes or dislikes the business. \n",
    "- Once we have numerical ratings representing each user’s feelings, we could track changes of average score over time. We could even automatically trigger an action whenever someone posts something very negative about the business. Free start-up idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxZmCwTUJRMf"
   },
   "source": [
    "## Why does this work? It seems too simple!\n",
    "\n",
    "- On it’s face, using text classification to understand text sounds like magical thinking. With a traditional NLP pipeline, we have to do a lot of work to understand the grammatical structure of text. \n",
    "- With a classifier, we’re just throwing huge buckets of text into a wood chipper and hoping for the best. Isn’t human expression more nuanced and complex than that? This is the kind of over-hyping and over simplification that makes machine learning look bad, right?\n",
    "\n",
    "\n",
    "- There’s several **reasons why treating text as a classification problem instead of as an understanding problem tends to work really well** — even when using relatively simple linear classification models.\n",
    "\n",
    "    - First, **people constantly create and evolve language.** Especially in an online word full of memes and emoji, writing code to reliably parse tweets and user reviews is going to be pretty difficult.\n",
    "    - With text classification, **the algorithm doesn’t care whether the user wrote standard English, an emoji, or a reference to Goku**. **The algorithm is looking for statistical relationships between input phrases and outputs.** If writing ಠ_ಠ correlates more heavily with 1-star and 2-star reviews, the algorithm will pick that up even though it has no idea what a “look of disapproval” emoticon is. \n",
    "    - The classifier can still figure out **what characters mean in the context of where they appear and how often they contribute to a particular output.\n",
    "    - Second, **website users don’t always write in the specific language that you expect.** An NLP pipeline trained to handle American English is going to fall apart if you give it German text. It’s also going to do poorly if your user decides to write their reviews with Cockney Rhyming Slang — which is still technically English.\n",
    "    - Again, a  **classification algorithm doesn’t care what language the text is in as long as it can at least break apart the text into separate words and measure the effects of those words. As long as you give the classifier enough training data to cover a wide range of possible English and German user reviews**, it will learn to handle both just fine.\n",
    "    - And finally, a big reason that text classification is so great is because it is **fast**. **Because linear text classification algorithms are so simple (compared to more complex machine learning models like recurrent neural networks), they can be trained quickly.** You can train a linear classifier with gigabytes of text in minutes on a regular laptop. You don’t even need any fancy hardware like a GPU. So even if you can get a slightly better accuracy score with a different machine learning algorithm, sometimes the tradeoff isn’t worth it. And research has shown that often the accuracy gap is nearly zero anyway.\n",
    "\n",
    "\n",
    "- While text classification models are simple to set up, that’s not to say they are always easy to get working well. The big catch is that you need **a lot of training data**. If you don’t have enough training data to cover the wide range of the ways that people write things, the model won’t ever be very accurate. \n",
    "- **The more training data you can collect, the better the model will perform.** The real art of applying text classification well is in finding clever ways of automatically collecting or creating training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DpBI3x7JRMf"
   },
   "source": [
    "## What can you do with Text Classification?\n",
    "\n",
    "- We’ve seen that we can use text classification to automatically score a user’s review text. That’s a type of sentiment analysis. Sentiment analysis is where you look at text that a user wrote and you try to figure out if the user is feeling positive or negative.\n",
    "\n",
    "\n",
    "- There’s lots of other practical uses of text classification. One that you probably use every day as a consumer without knowing it is the **email spam filtering** feature built into your email service. If you have a group of real emails marked as “spam” or “not spam”, you can use those to train a classification model that automatically flags spam emails in the future:\n",
    "![](https://miro.medium.com/max/2058/1*x93sfuRvT_bMOMLZHgEGLw.png)\n",
    "\n",
    "- Along the lines of spam filtering, you can also use text classification to identify abusive or obscene content and flag it. A lot of websites use text classification as a first-line defense against abusive users. By also taking the model’s confidence score into consideration, you can automatically block the worst offenders while sending the less certain cases to a human moderator to evaluate.\n",
    "\n",
    "- You can expand the idea of filtering beyond spam and abuse. More and more companies use use of text classification to **route support tickets**. The goal is to parse support questions from users and route them to the right team based on the kind of issue that the user is most likely reporting:\n",
    "\n",
    "![](https://miro.medium.com/max/2058/1*CljeC-ILeDX81P0Q9G1lGw.png)\n",
    "\n",
    "- By using classification to automate the busy work of triaging support tickets, the team is freed up to spend more time actually answering questions.\n",
    "\n",
    "- Text classification models can also be used to **categorize pretty much anything.** You can assume that any time you post on Facebook, behind the scenes it is classifying your post into categories like “family-related” or “related to a scheduled event”:\n",
    "![](https://miro.medium.com/max/2058/1*VFjLtmrZkg7N0CX6PBl7Ug.png)\n",
    "\n",
    "- That not only helps Facebook know which content to show to which users, but it also lets them track the topics that you are most interested in for advertising purposes.\n",
    "\n",
    "- Classification is also useful for **sorting and labeling documents**. Imagine that your company has done thousands of consulting projects for clients but that your boss wants them all re-organized according to a new government-mandated project coding system. Instead of reading through every project’s summary document and trying to decide which project code is the best match, you could classify a random sampling of them by hand and then build a classification model to automatically code the remaining ones:\n",
    "\n",
    "![](https://miro.medium.com/max/2058/1*bEKGFSNOwa-sY_Sk0WSQ4A.png)\n",
    "\n",
    "- These are just a few ideas. The uses of text classification are endless. You just have to figure out a way to reframe the problem so that the information you are trying to extract from the text can be mapped into a set of discrete output classes.\n",
    "\n",
    "- You can even build systems where one classification model feeds into another classification model. Imagine a user support system where the first classifier guesses the user’s language (English or German), the second classifier guesses which team is best suited to handle their request and a third classifier guesses whether or not the user is already upset to choose a ticket priority code. You can get as complex as you want!\n",
    "\n",
    "\n",
    "- Now that you are convinced of the awesomeness of dumb text classification models, let’s learn exactly how to build them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pSxdIFHJRMf"
   },
   "source": [
    "## Building the User Review Model With FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZTuwkHCJRMf"
   },
   "source": [
    "Here’s a simple piece of Python code that will read the reviews.json file and write out a text file in fastText format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc-HyCmeJRMf"
   },
   "source": [
    "## Step 1: Download Training Data\n",
    "\n",
    "- To build a user review model, we need training data. Luckily, Yelp provides a research dataset of 4.7 million user reviews. You can download it [here](https://www.yelp.com/dataset/download) (but keep in mind that you can’t use this data to build commercial applications).\n",
    "- When you download the data, you’ll get a 4 gigabyte json file called reviews.json. Each line in the file is a json object with data like this:\n",
    "\n",
    "{\n",
    "  \"review_id\": \"abc123\",\n",
    "  \"user_id\": \"xyy123\",\n",
    "  \"business_id\": \"1234\",\n",
    "  \"stars\": 5,\n",
    "  \"date\":\" 2015-01-01\",\n",
    "  \"text\": \"This restaurant is great!\",\n",
    "  \"useful\":0,\n",
    "  \"funny\":0,\n",
    "  \"cool\":0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqR06rCDJRMf"
   },
   "source": [
    "## Step 1-1: fastText 설치\n",
    "\n",
    "More detailed training: https://fasttext.cc/docs/en/supervised-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIm8QxcaPkCw",
    "outputId": "45c0d8d9-e5bb-4e44-bc8d-cdd45b7e7d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fastText' already exists and is not an empty directory.\n",
      "/content/fastText\n",
      "make: Nothing to be done for 'opt'.\n"
     ]
    }
   ],
   "source": [
    "# Building fastText as a command line tool\n",
    "!git clone https://github.com/facebookresearch/fastText.git\n",
    "%cd fastText\n",
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-LzbDqeQK9M",
    "outputId": "bdcac03b-9899-43c8-9904-cce908ce1dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF6FwL9uJRMg"
   },
   "source": [
    "## Step 2: Format and Pre-process Training Data\n",
    "\n",
    "- The first step is to convert this file into the format that fastText expects.\n",
    "- fastText requires a text file with each piece of text on a line by itself. The beginning of each line needs to have a special prefix of __label__YOURLABEL that assigns the label to that piece of text.\n",
    "- In other words, our restaurant review data needs to be reformatted like this:\n",
    "\n",
    "    - __label__5 This restaurant is great!\n",
    "    - __label__1 This restaurant is terrible :'(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhUlHLZBVILC",
    "outputId": "1c796e68-6795-4d25-9b14-4d65afe0321d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Yelp-Data-Challenge-2013' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# 리뷰 데이터 준비\n",
    "!git clone https://github.com/rekiksab/Yelp-Data-Challenge-2013.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D84LlYH0JRMg"
   },
   "outputs": [],
   "source": [
    "# Note: This example code is written for Python 3.6+!\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "#you have to change your data directory\n",
    "reviews_data = Path(\"Yelp-Data-Challenge-2013/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json\")\n",
    "fasttext_data = Path(\"fasttext_dataset.txt\")\n",
    "\n",
    "with reviews_data.open() as input, fasttext_data.open(\"w\") as output:\n",
    "    for line in input:\n",
    "        try:\n",
    "          review_data = json.loads(line)\n",
    "          rating = review_data['stars']\n",
    "          text = review_data['text'].replace(\"\\n\", \" \")\n",
    "\n",
    "          fasttext_line = \"__label__{} {}\".format(rating, text)\n",
    "\n",
    "          output.write(fasttext_line + \"\\n\")\n",
    "        except:\n",
    "          pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16uVGS2jJRMg",
    "outputId": "47b896bc-209d-4edc-ef50-7a65c8afbdfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp-Data-Challenge-2013/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json\n",
      "review_data:  {'votes': {'funny': 0, 'useful': 5, 'cool': 2}, 'user_id': 'rLtl8ZkDX5vH5nAx9C3q5Q', 'review_id': 'fWKvX83p0-ka4JS3dc6E5A', 'stars': 5, 'date': '2011-01-26', 'text': 'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!', 'type': 'review', 'business_id': '9yKzy9PApeiPPOUJEtnvkg'}\n",
      "\n",
      "\n",
      "fasttext_line:  __label__5 My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.  Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.  While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.  Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# input, output 형태 확인\n",
    "\n",
    "print(reviews_data)\n",
    "\n",
    "with reviews_data.open() as input, fasttext_data.open(\"w\") as output:\n",
    "    for line in input:\n",
    "        review_data = json.loads(line)\n",
    "        \n",
    "        print('review_data: ', review_data)\n",
    "\n",
    "        rating = review_data['stars']\n",
    "        text = review_data['text'].replace(\"\\n\", \" \")\n",
    "\n",
    "        fasttext_line = \"__label__{} {}\".format(rating, text)\n",
    "\n",
    "        # output.write(fasttext_line + \"\\n\")\n",
    "        \n",
    "        print('\\n')\n",
    "        print('fasttext_line: ', fasttext_line)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rTGbcpyJRMh"
   },
   "source": [
    "Data parsing code that reads the Yelp dataset, removes any string formatting and writes out separate training and test files. \n",
    "It randomly splits out 90% of the data as test data and 10% as test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfWPYkMuJRMh"
   },
   "source": [
    "- Running this creates a new file called fasttext_dataset.txt that we can feed into fastText for training. We aren’t done yet, though. We still need to do some additional pre-processing.\n",
    "\n",
    "- fastText is totally oblivious to any English language conventions (or the conventions of any other language). As far is it knows, the words Hello, hello and hello! are all totally different words because they aren’t exactly the same characters. To fix this, we want to do a quick pass through our text to convert everything to lowercase and to put spaces before punctuation marks. This is called text normalization and it makes it a lot easier for fastText to pick up on statistical patterns in the data.\n",
    "\n",
    "- This means that the textThis restaurant is great! should becomethis restaurant is great !.\n",
    "- **strip formatting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm9WHXzmJRMh"
   },
   "source": [
    "## Step 3: Split the data into a Training set and a Test set\n",
    "\n",
    "- To get an accurate measure of how well our model performs, we need to test it’s ability to classify text using text that it didn’t see during training. If we test it against the training data, it is like giving it an open book test where it can memorize the answers.\n",
    "- So we need to extract some of the strings from the training data set and keep them in separate test data file. Then we can test the trained model’s performance with that held-back data to get a real-world measure of how well the model performs.\n",
    "- Here’s a final version of our data parsing code that reads the Yelp dataset, removes any string formatting and writes out separate training and test files. It randomly splits out 90% of the data as test data and 10% as test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RvU_jnUHJRMh"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "\n",
    "reviews_data = Path(\"Yelp-Data-Challenge-2013/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json\")\n",
    "training_data = Path(\"fasttext_dataset_training.txt\")\n",
    "test_data = Path(\"fasttext_dataset_test.txt\")\n",
    "\n",
    "# What percent of data to save separately as test data\n",
    "percent_test_data = 0.10\n",
    "\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "with reviews_data.open() as input, \\\n",
    "     training_data.open(\"w\") as train_output, \\\n",
    "     test_data.open(\"w\") as test_output:\n",
    "\n",
    "    for line in input:\n",
    "\n",
    "      try:\n",
    "        review_data = json.loads(line)\n",
    "\n",
    "        rating = review_data['stars']\n",
    "        text = review_data['text'].replace(\"\\n\", \" \")\n",
    "        text = strip_formatting(text)\n",
    "\n",
    "        fasttext_line = \"__label__{} {}\".format(rating, text)\n",
    "\n",
    "        if random.random() <= percent_test_data:\n",
    "            test_output.write(fasttext_line + \"\\n\")\n",
    "        else:\n",
    "            train_output.write(fasttext_line + \"\\n\")\n",
    "\n",
    "      except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj82NpdLJRMh"
   },
   "source": [
    "- Run that and you’ll have two files,fasttext_dataset_training.txt and fasttext_dataset_test.txt. Now we are ready to train!\n",
    "\n",
    "- Here’s one more tip though: To make your model robust, you will also want to randomize the order of lines in each data file so that the order of the training data doesn’t influence the training process. That’s not absolutely required in this case since the data from Yelp is already pretty random, but it’s definitely worth doing when using your own data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8iyq0qpJRMi"
   },
   "source": [
    "## Step 4: Train the Model\n",
    "\n",
    "- You can train a classifier using the fastText command line tool. You just call fasttext, pass in the supervised keyword to tell it train a supervised classification model, and then give it the training file and and an output name for the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bvKuymw5JRMi",
    "outputId": "f076c8ed-6de0-46ec-e53d-e9c6f391d99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 32M words\n",
      "Number of words:  194033\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread:  548727 lr:  0.000000 avg.loss:  0.986350 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "!fastText/fasttext supervised -input fasttext_dataset_training.txt -output reviews_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDd1cK_RJRMi"
   },
   "source": [
    "## Step 5: Test the Model\n",
    "Let’s see how accurate the model is by checking it against our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90gn3K37JRMi",
    "outputId": "0ef77578-2d69-47e2-940c-339505181e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t22976\n",
      "P@1\t0.6\n",
      "R@1\t0.6\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "!fastText/fasttext test reviews_model.bin fasttext_dataset_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n8SzpUjJRMi"
   },
   "source": [
    "- This means that across 667.986 examples, it guessed the user’s exact star rating 69.7% of the time. Not a bad start.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI42wW4LJRMj"
   },
   "source": [
    "## Precision, Recall and Accuracy?\n",
    "- https://sumniya.tistory.com/26\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99DC064C5BE056CE10)\n",
    "\n",
    "- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)\n",
    "- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)\n",
    "- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)\n",
    "- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)\n",
    "\n",
    "###  Precision(정밀도)\n",
    "정밀도란 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율입니다. 즉, 아래와 같은 식으로 표현할 수 있습니다. 여기서는 한달 동안의 날씨를 예측하는 상황을 생각해보겠습니다. 날씨는 비가 오거나 맑거나 두 가지만 존재한다고 가정합니다.\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99F66B345BE0596109)\n",
    "\n",
    "**Positive 정답률, PPV(Positive Predictive Value)** 라고도 불립니다. 날씨 예측 모델이 맑다로 예측했는데, 실제 날씨가 맑았는지를 살펴보는 지표라고 할 수 있겠습니다.\n",
    "\n",
    "### Recall(재현율)\n",
    "재현율이란 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율입니다. \n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/997188435BE05B0628)\n",
    "\n",
    "통계학에서는 **sensitivity**으로, 그리고 다른 분야에서는 **hit rate** 라는 용어로도 사용합니다.\n",
    "\n",
    "즉, Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, 바라보고자 하는 관점만 다릅니다. Precision은 모델의 입장에서, 그리고 Recall은 실제 정답(data)의 입장에서 정답을 정답이라고 맞춘 경우를 바라보고 있습니다.\n",
    "\n",
    "###  Accuracy(정확도)\n",
    "위 두 지표는 모두 True를 True라고 옳게 예측한 경우에 대해서만 다루었습니다. 하지만, False를 False라고 예측한 경우도 옳은 경우입니다. 이때, 해당 경우를 고려하는 지표가 바로 정확도(Accuracy)입니다\n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/99745F3F5BE0613D1A)\n",
    "\n",
    "###  F1 score\n",
    "F1 score는 Precision과 Recall의 조화평균입니다. \n",
    "\n",
    "![](https://t1.daumcdn.net/cfile/tistory/993482335BE0641515)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ohty4Y9DJRMj"
   },
   "source": [
    "- You can also ask fastText to check how often the correct star rating was in one of it’s Top 2 predictions (i.e. if the model’s top two most likely guesses were “5”, “4” and the real user said “4”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JE8VeZTJRMj",
    "outputId": "fe76d3e2-d38e-4916-87b7-2b33e64e9d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t22976\n",
      "P@2\t0.442\n",
      "R@2\t0.884\n"
     ]
    }
   ],
   "source": [
    "#Two best guesses\n",
    "!fastText/fasttext test reviews_model.bin fasttext_dataset_test.txt 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8djll5WCJRMj"
   },
   "source": [
    "- That means that 91.6% of the time, it recalled the user’s star rating if we check its two best guesses. That’s a good indication that the model is not far off in most cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08me3UJ2JRMj"
   },
   "source": [
    "- You can also try out the model interactively by running the fasttext predict command and then typing in your own reviews. When you hit enter, it will tell you its prediction for each one:\n",
    "\n",
    "- fasttext predict reviews_model.bin -\n",
    "- this is a terrible restaurant . i hate it so much .\n",
    "- __label__1\n",
    "- this is a very good restaurant .\n",
    "- __label__4\n",
    "- this is the best restaurant i have ever tried .\n",
    "- __label__5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xAmrHuwGQ42B"
   },
   "outputs": [],
   "source": [
    "testfile = open('fastTexttest.txt', 'w')\n",
    "testfile.write('this is a terrible restaurant . i hate it so much .' + '\\n')\n",
    "testfile.write('this is a very good restaurant .' + '\\n')\n",
    "testfile.write('this is the best restaurant i have ever tried .' + '\\n')\n",
    "testfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrZPG-qBJRMj",
    "outputId": "801e8456-7767-4a14-eb60-394244a43985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1\n",
      "__label__4\n",
      "__label__5\n"
     ]
    }
   ],
   "source": [
    "#fastTexttest.txt contains above three sentences\n",
    "!fastText/fasttext predict reviews_model.bin fastTexttest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mWe6OTHJRMk"
   },
   "source": [
    "## Step 6: Iterate on the model to make it more accurate\n",
    "\n",
    "- With the default training settings, fastText tracks each word independently and doesn’t care at all about word order. But when you have a large training data set, you can ask it to take the order of words into consideration by using the wordNgrams parameter. That will make it track groups of words instead of just individual words.\n",
    "\n",
    "- For a data set of millions of words, tracking two word pairs (also called bigrams) instead of single words is a good starting point for improving the model.\n",
    "\n",
    "- Let’s train a new model with the -wordNgrams 2 parameter and see how it performs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUjlhl9WJRMk",
    "outputId": "ec6c7e6d-8284-4111-f961-33533e015c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 32M words\n",
      "Number of words:  194033\n",
      "Number of labels: 5\n",
      "Progress: 100.0% words/sec/thread:  245385 lr:  0.000000 avg.loss:  0.896788 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "#Train Using Bigram(Ngram2)\n",
    "!fastText/fasttext supervised -input fasttext_dataset_training.txt -output reviews_model_ngrams -wordNgrams 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWTFCMhlJRMk"
   },
   "source": [
    "- This will make training take a bit longer and it will make the model file much larger (since there is now an entry for every two-word pair in the data), but it can be worth it if it gives us higher accuracy.\n",
    "- Once the training completes, you can re-run the test command the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWwl2Vg4JRMk",
    "outputId": "ea1ddcf8-8a6b-4964-c0cf-c570db7138ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t22976\n",
      "P@1\t0.625\n",
      "R@1\t0.625\n"
     ]
    }
   ],
   "source": [
    "#rerun test program\n",
    "!fastText/fasttext test reviews_model_ngrams.bin fasttext_dataset_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzQxknYuJRMl"
   },
   "source": [
    "- For me, using -wordNgrams 2 got me to 71.2% accuracy on the test set, an improvement of nearly 4%. It also seems to reduce the number of obvious errors that the model makes because now it cares a little bit about the context of each word.\n",
    "- There are other ways to improve your model, too. One of the simplest but most effective ways is skim your training data file by hand and make sure that the preprocessing code is formatting your text in a sane way.\n",
    "- For example, my sample text pre-processing code will turn the common restaurant nameP.F. Chang into p . f . chang. That appears as five separate words to fastText.\n",
    "- If you have cases like that where important words that represent a single concept are getting split up, you can write custom code to fix it. In this case, you might add code to look for common restaurant names and replace them with placeholders like p_f_chang so that fastText sees each as a single word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fPGZIiHJRMl"
   },
   "source": [
    "## Step 7: Use your model in your program!\n",
    "\n",
    "- The best part about fastText is that it’s easy to call a trained model from any Python program.\n",
    "- There are a few different Python wrappers for fastText that you can use, but I like the official one created by Facebook. You can install it by following these directions.\n",
    "- With that installed, here’s the entire code to load the model and use it to automatically score user reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcBBY82ySX6O",
    "outputId": "4e932f1d-07d1-4de4-d225-a889e3d1ab7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/fastText\n",
      "Processing /content/fastText\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (2.7.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.19.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3083913 sha256=428001d5aba61544ac9232e2ed5363787e542bfa006512b206eb1f55904cea2c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o0pz_lrf/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
      "Successfully built fasttext\n",
      "Installing collected packages: fasttext\n",
      "  Attempting uninstall: fasttext\n",
      "    Found existing installation: fasttext 0.9.2\n",
      "    Uninstalling fasttext-0.9.2:\n",
      "      Successfully uninstalled fasttext-0.9.2\n",
      "Successfully installed fasttext-0.9.2\n",
      "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "  % (opt, underscore_opt))\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing python/fasttext_module/fasttext.egg-info/PKG-INFO\n",
      "writing dependency_links to python/fasttext_module/fasttext.egg-info/dependency_links.txt\n",
      "writing requirements to python/fasttext_module/fasttext.egg-info/requires.txt\n",
      "writing top-level names to python/fasttext_module/fasttext.egg-info/top_level.txt\n",
      "reading manifest template 'MANIFEST.in'\n",
      "warning: no files found matching 'PATENTS'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'python/fasttext_module/fasttext.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "running build_ext\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c /tmp/tmp9klaox7o.cpp -o tmp/tmp9klaox7o.o -std=c++11\n",
      "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-LSlbJj/python3.7-3.7.11=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c /tmp/tmp_bussa3r.cpp -o tmp/tmp_bussa3r.o -fvisibility=hidden\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-3.7/fasttext_pybind.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/fasttext\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/__init__.py -> build/bdist.linux-x86_64/egg/fasttext\n",
      "creating build/bdist.linux-x86_64/egg/fasttext/tests\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/tests/test_configurations.py -> build/bdist.linux-x86_64/egg/fasttext/tests\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/tests/test_script.py -> build/bdist.linux-x86_64/egg/fasttext/tests\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/tests/__init__.py -> build/bdist.linux-x86_64/egg/fasttext/tests\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/FastText.py -> build/bdist.linux-x86_64/egg/fasttext\n",
      "creating build/bdist.linux-x86_64/egg/fasttext/util\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/util/__init__.py -> build/bdist.linux-x86_64/egg/fasttext/util\n",
      "copying build/lib.linux-x86_64-3.7/fasttext/util/util.py -> build/bdist.linux-x86_64/egg/fasttext/util\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/tests/test_configurations.py to test_configurations.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/tests/test_script.py to test_script.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/tests/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/FastText.py to FastText.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext/util/util.py to util.cpython-37.pyc\n",
      "creating stub loader for fasttext_pybind.cpython-37m-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fasttext_pybind.py to fasttext_pybind.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying python/fasttext_module/fasttext.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying python/fasttext_module/fasttext.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying python/fasttext_module/fasttext.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying python/fasttext_module/fasttext.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying python/fasttext_module/fasttext.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying python/fasttext_module/fasttext.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "creating 'dist/fasttext-0.9.2-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing fasttext-0.9.2-py3.7-linux-x86_64.egg\n",
      "removing '/usr/local/lib/python3.7/dist-packages/fasttext-0.9.2-py3.7-linux-x86_64.egg' (and everything under it)\n",
      "creating /usr/local/lib/python3.7/dist-packages/fasttext-0.9.2-py3.7-linux-x86_64.egg\n",
      "Extracting fasttext-0.9.2-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
      "fasttext 0.9.2 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /usr/local/lib/python3.7/dist-packages/fasttext-0.9.2-py3.7-linux-x86_64.egg\n",
      "Processing dependencies for fasttext==0.9.2\n",
      "Searching for numpy==1.19.5\n",
      "Best match: numpy 1.19.5\n",
      "Adding numpy 1.19.5 to easy-install.pth file\n",
      "Installing f2py script to /usr/local/bin\n",
      "Installing f2py3 script to /usr/local/bin\n",
      "Installing f2py3.7 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for setuptools==57.4.0\n",
      "Best match: setuptools 57.4.0\n",
      "Adding setuptools 57.4.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for pybind11==2.7.1\n",
      "Best match: pybind11 2.7.1\n",
      "Adding pybind11 2.7.1 to easy-install.pth file\n",
      "Installing pybind11-config script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Finished processing dependencies for fasttext==0.9.2\n"
     ]
    }
   ],
   "source": [
    "# Building fasttext python module\n",
    "%cd fastText\n",
    "!pip install .\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hfn1d6KgSojC",
    "outputId": "91ce0e37-5713-4f5c-a71c-dfc465b2462a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgsixR8MJRMl",
    "outputId": "7457263b-1024-4adf-b322-92a8057d6350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☆☆☆☆☆ (99% confidence)\n",
      "This restaurant literally changed my life. This is the best food I've ever eaten!\n",
      "\n",
      "☆ (74% confidence)\n",
      "I hate this place so much. They were mean to me.\n",
      "\n",
      "☆☆☆ (52% confidence)\n",
      "I don't know. It was ok, I guess. Not really sure what to say.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import re\n",
    "\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "# Reviews to check\n",
    "reviews = [\n",
    "    \"This restaurant literally changed my life. This is the best food I've ever eaten!\",\n",
    "    \"I hate this place so much. They were mean to me.\",\n",
    "    \"I don't know. It was ok, I guess. Not really sure what to say.\"\n",
    "]\n",
    "\n",
    "# Pre-process the text of each review so it matches the training format\n",
    "preprocessed_reviews = list(map(strip_formatting, reviews))\n",
    "\n",
    "# Load the model\n",
    "classifier = fasttext.load_model('reviews_model_ngrams.bin')\n",
    "\n",
    "# Get fastText to classify each review with the model\n",
    "labels, probabilities = classifier.predict(preprocessed_reviews, 1)\n",
    "\n",
    "# Print the results\n",
    "for review, label, probability in zip(reviews, labels, probabilities):\n",
    "    stars = int(label[0][-1])\n",
    "\n",
    "    print(\"{} ({}% confidence)\".format(\"☆\" * stars, int(probability[0] * 100)))\n",
    "    print(review)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMECeOf2JRMl"
   },
   "source": [
    "## Natural Language Processing Is Fun Part 3: Explaining Model Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6X_pi6mbTdBw",
    "outputId": "6bd7bc32-4178-4781-ba4e-d79c2e98a00e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime in /usr/local/lib/python3.7/dist-packages (0.2.0.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime) (0.16.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lime) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from lime) (4.62.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lime) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from lime) (0.22.2.post1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (7.1.2)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime) (2.6.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->lime) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqQd5dJEJRMl",
    "outputId": "c5597366-8c85-4622-eae2-6c9724a6b6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/explanation.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import lime.lime_text\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "\n",
    "# This function regularizes a piece of text so it's in the same format\n",
    "# that we used when training the FastText classifier.\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "# LIME needs to be able to mimic how the classifier splits\n",
    "# the string into words. So we'll provide a function that\n",
    "# mimics how FastText works.\n",
    "def tokenize_string(string):\n",
    "    return string.split()\n",
    "\n",
    "# Load our trained FastText classifier model (created in Part 2)\n",
    "classifier = fasttext.load_model('reviews_model_ngrams.bin')\n",
    "\n",
    "# Create a LimeTextExplainer. This object knows how to explain a text-based\n",
    "# prediction by dropping words randomly.\n",
    "explainer = lime.lime_text.LimeTextExplainer(\n",
    "    # We need to tell LIME how to split the string into words. We can do this\n",
    "    # by giving it a function to call to split a string up the same way FastText does it.\n",
    "    split_expression=tokenize_string,\n",
    "    # Our FastText classifer uses bigrams (two-word pairs) to classify text. Setting\n",
    "    # bow=False tells LIME to not assume that our classifier is based on single words only.\n",
    "    bow=False,\n",
    "    # To make the output pretty, tell LIME what to call each possible prediction from our model.\n",
    "    class_names=[\"No Stars\", \"1 Star\", \"2 Stars\", \"3 Stars\", \"4 Stars\", \"5 Stars\"]\n",
    ")\n",
    "\n",
    "# LIME is designed to work with classifiers that generate predictions\n",
    "# in the same format as Scikit-Learn. It expects every prediction to have\n",
    "# a probability value for every possible label.\n",
    "# The default FastText python wrapper generates predictions in a different\n",
    "# format where it only returns the top N highest likelihood results. This\n",
    "# code just calls the FastText predict function and then massages it into\n",
    "# the format that LIME expects (so that LIME will work).\n",
    "def fasttext_prediction_in_sklearn_format(classifier, texts):\n",
    "    res = []\n",
    "    # Ask FastText for the top 10 most likely labels for each piece of text.\n",
    "    # This ensures we always get a probability score for every possible label in our model.\n",
    "    labels, probabilities = classifier.predict(texts, 10)\n",
    "\n",
    "    # For each prediction, sort the probabaility scores into the same order\n",
    "    # (I.e. no_stars, 1_star, 2_star, etc). This is needed because FastText\n",
    "    # returns predicitons sorted by most likely instead of in a fixed order.\n",
    "    for label, probs, text in zip(labels, probabilities, texts):\n",
    "        order = np.argsort(np.array(label))\n",
    "        res.append(probs[order])\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "# Review to explain\n",
    "review = \"I didn't love this place :( The food wasn't very good and I didn't like the service either. Also, I found a bug in my food.\"\n",
    "\n",
    "# Pre-process the text of the review so it matches the training format\n",
    "preprocessed_review = strip_formatting(review)\n",
    "\n",
    "# Make a prediction and explain it!\n",
    "exp = explainer.explain_instance(\n",
    "    # The review to explain\n",
    "    preprocessed_review,\n",
    "    # The wrapper function that returns FastText predictions in scikit-learn format\n",
    "    classifier_fn=lambda x: fasttext_prediction_in_sklearn_format(classifier, x),\n",
    "    # How many labels to explain. We just want to explain the single most likely label.\n",
    "    top_labels=1,\n",
    "    # How many words in our sentence to include in the explanation. You can try different values.\n",
    "    num_features=20,\n",
    ")\n",
    "\n",
    "# Save the explanation to an HTML file so it's easy to view.\n",
    "# You can also get it to other formats: as_list(), as_map(), etc.\n",
    "# See https://lime-ml.readthedocs.io/en/latest/lime.html#lime.explanation.Explanation\n",
    "#output_filename = \"explanation.html\"\n",
    "output_filename = Path('__file__').resolve().parent / \"explanation.html\"\n",
    "print(output_filename)\n",
    "exp.save_to_file(output_filename)\n",
    "\n",
    "# Open the explanation html in our web browser.\n",
    "#output_filename = 'file:///' + output_filename\n",
    "webbrowser.open(output_filename.as_uri())\n",
    "#webbrowser.open(\"file:///explanation.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6TzVZmGiJRMl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "z_oUeLujJRMX",
    "28wP2B3UJRMX",
    "BlBTufCPJRMX",
    "jN4QyucIJRMX",
    "KnPC6h68JRMY",
    "MUXmu8dTJRMZ",
    "2hL72hRfJRMZ",
    "Z7wZpRH9JRMa",
    "OyF2xIq4JRMa",
    "wN-oa6wsJRMb",
    "cAWX2Uq4JRMe",
    "q8P2loY9JRMe",
    "lxZmCwTUJRMf",
    "4DpBI3x7JRMf",
    "yc-HyCmeJRMf"
   ],
   "name": "NaturalLanguageProcessingIsFunCombined_yelp_실습_수정 .ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
