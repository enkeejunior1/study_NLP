{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 및 자연어 빅데이터 분석방법론/  컴퓨터언어학연구 I\n",
    "\n",
    "## String Similarity\n",
    "- based on https://itnext.io/string-similarity-the-basic-know-your-algorithms-guide-3de3d7346227\n",
    "- based on https://www.analyticsvidhya.com/blog/2021/02/a-simple-guide-to-metrics-for-calculating-string-similarity/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- One of the applications of Natural Language Processing is auto-correction and spellings checks. \n",
    "- All of us have encountered this that if we type an incorrect or typo in the Google search engine, then the engine automatically corrects it and suggests the right word in its place. \n",
    "- How does the engine do that? How does it know what word we wanted to write or ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the best string similarity algorithm? Well, it’s quite hard to answer this question, at least without knowing anything else, like what you require it for. And even after having a basic idea, it’s quite hard to pinpoint to a good algorithm without first trying them out on different datasets. \n",
    "- It’s a trial and error process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of algorithms\n",
    "\n",
    "Based on the properties of operations, string similarity algorithms can be classified into a bunch of domains. Let’s discuss a few of them,\n",
    "\n",
    "**Edit distance based:** Algorithms falling under this category try to compute the number of operations needed to transforms one string to another. More the number of operations, less is the similarity between the two strings. One point to note, in this case, every index character of the string is given equal importance.\n",
    "\n",
    "**Token-based:** In this category, the expected input is a set of tokens, rather than complete strings. The idea is to find the similar tokens in both sets. More the number of common tokens, more is the similarity between the sets. A string can be transformed into sets by splitting using a delimiter. This way, we can transform a sentence into tokens of words or n-grams characters. Note, here tokens of different length have equal importance.\n",
    "\n",
    "**Sequence-based:** Here, the similarity is a factor of common sub-strings between the two strings. The algorithms, try to find the longest sequence which is present in both strings, the more of these sequences found, higher is the similarity score. Note, here combination of characters of same length have equal importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit distance based algorithms\n",
    "\n",
    "Let’s try to understand most widely used algorithms within this type,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming distance\n",
    "Hamming Distance, named after the American mathematician, is the simplest algorithm for calculating string similarity. It checks the similarity by comparing **the changes in the number of positions between the two strings.** The method compares each and every letter of one string with the corresponding letter of the other string.\n",
    "\n",
    "Let’s take these two words: TIME and MINE. Every letter of the two strings will be compared in the following way:\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/40331time%20mine%20hd.JPG\" witdth=\"60%\" align=center>\n",
    "\n",
    "- The first two letters T and M are different so the number of positions in which the two strings are different is 1 up until now.\n",
    "\n",
    "- The next two letters ‘I’ are the same, hence the number of positions at which two strings are different is 0.\n",
    "\n",
    "- Now, the next two letters M and N are again different so the number of positions of two strings being different is 1, and\n",
    "\n",
    "- The last two letters E are the same so the number of positions of two strings different is 0.\n",
    "\n",
    "This way the Hamming distance is 2 = 1 + 0 + 1 + 0. It is the total number of positions different between two strings at each character’s place. In short, the number of unequal characters is equal to the Hamming distance.\n",
    "\n",
    "The Hamming distance can range anywhere between 0 and any integer value, even equal to the length of the string. For this, we can also normalize the value by taking the ratio of the Hamming distance to the length of the string in the following manner:\n",
    "\n",
    "**Normalized Hamming Distance = Hamming Distance/ length of the string**\n",
    "\n",
    "**Normalized Hamming distance gives the percentage to which the two strings are dissimilar.** The normalized Hamming distance for the above TIME and MINE example is: 2/4 = 0.50, hence 50% of these two characters are not similar. **A lower value of Normalized Hamming distance means the two strings are more similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "print(textdistance.hamming('text', 'test'))\n",
    "print(textdistance.hamming.normalized_similarity('text', 'test'))\n",
    "print(textdistance.hamming('arrow', 'arow'))\n",
    "print(textdistance.hamming.normalized_similarity('arrow', 'arow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As evident, in first example, the two strings vary only at the 3rd position, hence the edit distance is 1. \n",
    "- In second example, even though we are only missing one ‘r’, the ‘row’ part is offset by 1, making the edit distance 3 (3rd, 4th and 5th position are dissimilar). \n",
    "- One thing to note is the normalized similarity, this is nothing but a function to bound the edit distance between 0 and 1. \n",
    "- This signifies, if the score is 0-two strings cannot be more dissimilar, on the other hand, a score of 1 is for a perfect match. \n",
    "- So the strings in first example are 75% similar (expected) but in strings in second example are only 40% similar (can we do better?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein distance\n",
    "- This distance is computed by finding the number of edits which will transform one string to another. \n",
    "- The transformations allowed are insertion — adding a new character, deletion — deleting a character and substitution — replace one character by another. \n",
    "- By performing these three operations, the algorithm tries to modify first string to match the second one. In the end we get a edit distance. Examples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textdistance.levenshtein('arrow', 'arow'))\n",
    "print(textdistance.levenshtein.normalized_similarity('arrow', 'arow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- distance is 1. Similar with hamming distance, we can generate a bounded similarity score between 0 and 1. \n",
    "- The similarity score is 80%, huge improvement over the last algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro Similarity\n",
    "<img src=\"https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a94c9faafd2fc87f40b299816f9e35a0_l3.svg\" witdth=\"80%\" align=center>\n",
    "\n",
    "where:\n",
    "\n",
    "    - m is the number of matching characters\n",
    "    - t is half the number of transpositions\n",
    "    - where |s1| and |s2| are the lengths of strings s1 and s2 respectively.\n",
    "\n",
    "The characters are said to be matching if they are the same and the characters are not further than <img src=\"https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-81e12ffc6a72b15d2ff7a2f7b7ed3db1_l3.svg\" witdth=\"80%\"> characters apart.\n",
    "\n",
    "    - Let s1=”arnab”, s2=”raanb”, so the maximum distance to which each character is matched is 1.\n",
    "    - It is evident that both the strings have 5 matching characters, but the order is not the same, so the number of characters that are not in order is 4, so the number of transpositions is 2.\n",
    "    - Therefore, Jaro similarity can be calculated as follows: \n",
    "    - Jaro Similarity = (1/3) * $(5/5) + (5/5) + (5-2)/5 } = 0.86667\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-05%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2011.34.01.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro–Winkler Similarity\n",
    "Jaro–Winkler similarity uses a prefix scale $p$ which gives more favorable ratings to strings that match from the beginning for a set prefix length $l$. Given two strings ${s_{1}}$ and ${s_{2}}$, their Jaro–Winkler similarity ${sim_{w}}$ is:\n",
    "\n",
    "![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-05%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2011.34.01.png](attachment:%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-09-05%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%2011.34.01.png)\n",
    "\n",
    "where:\n",
    "\n",
    "- ${sim_{j}}$ is the Jaro similarity for strings ${s_{1}}$ and ${s_{2}}$\n",
    "- $l$  is the length of common prefix at the start of the string up to a maximum of 4 characters\n",
    "- $p$ is a constant scaling factor for how much the score is adjusted upwards for having common prefixes.\n",
    "- $p$ should not exceed 0.25 (i.e. 1/4, with 4 being the maximum length of the prefix being considered), otherwise the similarity could become larger than 1. \n",
    "- The standard value for this constant in Winkler's work is $p=0.1$\n",
    "\n",
    "The Jaro–Winkler distance ${d_{w}}$ is defined as ${d_{w}=1-sim_{w}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this code is not working, please refer to the texedistance document for external libraries\n",
    "# https://pypi.org/project/textdistance/   \n",
    "\n",
    "print(textdistance.jaro_winkler(\"mes\", \"messi\"))\n",
    "print(textdistance.jaro_winkler(\"crate\", \"crat\"))\n",
    "print(textdistance.jaro_winkler(\"crate\", \"atcr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead us this code .. Just ignore first external library use \n",
    "\n",
    "jaro = textdistance.JaroWinkler(external=False)\n",
    "print (jaro(\"mes\", \"messi\"))\n",
    "jaro = textdistance.JaroWinkler(external=False)\n",
    "print (jaro(\"crate\", \"crat\"))\n",
    "jaro = textdistance.JaroWinkler(external=False)\n",
    "print (jaro(\"crate\", \"atcr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In first case, as the strings were matching from the beginning, high score was provided. \n",
    "- Similarly, in the second case, only one character was missing and that too at the end of the string 2, hence a very high score was given. \n",
    "- Imagine the previous algorithms, the similarity would have been less, 80% to be exact. \n",
    "- In third case, we re-arranged the last two character of string 2, by bringing them at front, which resulted in 0% similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token based algorithms\n",
    "Algorithms falling under this category are more or less, set similarity algorithms, modified to work for the case of string tokens. Some of them are,\n",
    "\n",
    "### Jaccard index\n",
    "Falling under the set similarity domain, the formulae is to find the number of common tokens and divide it by the total number of unique tokens. Its expressed in the mathematical terms by,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/325/1*wDOEGSMvUMzHGC45tgLAcw.png\" witdth=\"80%\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, the numerator is the intersection (common tokens) and denominator is union (unique tokens). \n",
    "- The second case is for when there is some overlap, for which we must remove the common terms as they would add up twice by combining all tokens of both strings. \n",
    "- As the required input is tokens instead of complete strings, it falls to user to efficiently and intelligently tokenize his string, depending on the use case. Examples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_1 = \"hello world\".split()\n",
    "tokens_2 = \"world hello\".split()\n",
    "print(textdistance.jaccard(tokens_1 , tokens_2))\n",
    "tokens_1 = \"hello new world\".split()\n",
    "tokens_2 = \"hello world\".split()\n",
    "print(textdistance.jaccard(tokens_1 , tokens_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first tokenize the string by default space delimiter, to make words in the strings as tokens. Then we compute the similarity score. In first example, as both words are present in both the strings, the score is 1. Just imagine running an edit based algorithm in this case, the score will be very less if not 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorensen-Dice\n",
    "Falling under set similarity, the logic is to find the common tokens, and divide it by the total number of tokens present by combining both sets. The formulae is,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/158/1*hFlnw58WIUErvL2xPg00BA.png\" witdth=\"80%\" align=center>\n",
    "\n",
    "where, the numerator is twice the intersection of two sets/strings. The idea behind this is if a token is present in both strings, its total count is obviously twice the intersection (which removes duplicates). The denominator is simple combination of all tokens in both strings. Note, its quite different from the jaccard’s denominator, which was union of two strings. As the case with intersection, union too removes duplicates and this is avoided in dice algorithm. Because of this, dice will always overestimate the similarity between two strings. Some example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_1 = \"hello world\".split()\n",
    "tokens_2 = \"world hello\".split()\n",
    "print(textdistance.sorensen(tokens_1 , tokens_2))\n",
    "tokens_1 = \"hello new world\".split()\n",
    "tokens_2 = \"hello world\".split()\n",
    "print(textdistance.sorensen(tokens_1 , tokens_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence based algorithm\n",
    "\n",
    "Lets understand one of the sequence based algorithms,\n",
    "\n",
    "### Ratcliff-Obershelp similarity\n",
    "\n",
    "- The idea is quite simple yet intuitive. Find the longest common substring from the two strings.\n",
    "- Remove that part from both strings, and split at the same location. \n",
    "- This breaks the strings into two parts, one left and another to the right of the found common substring. \n",
    "- Now take the left part of both strings and call the function again to find the longest common substring. \n",
    "- Do this too for the right part. \n",
    "- This process is repeated recursively until the size of any broken part is less than a default value. \n",
    "- Finally, a formulation similar to the above-mentioned dice is followed to compute the similarity score. \n",
    "- The score is twice the number of characters found in common divided by the total number of characters in the two strings. Some examples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1, string2 = \"i am going home\", \"gone home\"\n",
    "print(textdistance.ratcliff_obershelp(string1, string2))\n",
    "string1, string2 = \"helloworld\", \"worldhello\"\n",
    "print(textdistance.ratcliff_obershelp(string1, string2))\n",
    "string1, string2 = \"test\", \"text\"\n",
    "print(textdistance.ratcliff_obershelp(string1, string2))\n",
    "string1, string2 = \"mes\", \"simes\"\n",
    "print(textdistance.ratcliff_obershelp(string1, string2))\n",
    "string1, string2 = \"arrow\", \"arow\"\n",
    "print(textdistance.ratcliff_obershelp(string1, string2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first example, it found ‘ home’ as the longest substring, then considered ‘i am going’ and ‘gone’ for further processing (left of common substring), where again it found ‘go’ as longest substring. Later on right of ‘go’ it also found ’n’ as the only common and longest substring. Overall the score was 2 * (5 + 2 + 1) / 24 ~ 0.66. In second case, it found ‘hello’ as the longest longest substring and nothing common on the left and right, hence score is 0.5. The rest of the examples showcase the advantage of using sequence algorithms for cases missed by edit distance based algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The selection of the string similarity algorithm depends on the use case. All of the above-mentioned algorithms, one way or another, try to find the common and non-common parts of the strings and factor them to generate the similarity score. And without complicating the procedure, majority of the use cases can be solved by using one of these algorithms. A little more complicated domains include vector representation and compression types, which also consider the semantic of the words or n-grams. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
