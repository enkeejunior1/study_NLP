{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "# Combine the stopwords. Its a lot longer so I'm not printing it out...\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)\n",
    "\n",
    "# Remove the stopwords from `single_no8`.\n",
    "#print('With combined stopwords:')\n",
    "#print([word for word in single_no8_tokenized_lowered if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "6fe6901c-88ca-438f-9a9a-b1f28128ffc4",
    "_uuid": "4f7803ff5cbc3f6c0dc97ce9c353a0bec39a5696"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "lemmatize_sent('He is walking to school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "358ca69f-e7b2-4b93-ad09-5cf1b0a7143a",
    "_uuid": "3145c9a5568eb2ed075eddf5a617d20b2286f012"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9b00681d-5757-4547-828f-5767bfe701bd",
    "_uuid": "3f40b6cb3f60bc2bbfeebd27de0802d0bf7849cf"
   },
   "source": [
    "# From Strings to Vectors\n",
    "\n",
    "**Vector** is an array of numbers\n",
    "\n",
    "**Vector Space Model** is conceptualizing language as a whole lot of numbers\n",
    "\n",
    "**Bag-of-Words (BoW)**: Counting each document/sentence as a vector of numbers, with each number representing the count of a word in the corpus\n",
    "\n",
    "To count, we can use the Python `collections.Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "37963ad4-b911-4bfc-b723-f6b7927a1eeb",
    "_uuid": "e5527ef7e753e6b3981c9909b50fcf39a428a85c"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "# Lemmatize and remove stopwords\n",
    "processed_sent1 = preprocess_text(sent1)\n",
    "processed_sent2 = preprocess_text(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "882d59c1-e3bf-4e72-84e2-858490ec66b7",
    "_uuid": "cfb48c07796e6b3fbe20378dcef1f11e4880b1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentence:\n",
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "\n",
      "Word counts:\n",
      "Counter({'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'lazy': 1, 'dog': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed sentence:')\n",
    "print(processed_sent1)\n",
    "print()\n",
    "print('Word counts:')\n",
    "print(Counter(processed_sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "75e94392-cf5c-40f5-bcd7-349e4861fab7",
    "_uuid": "4e9bd5f5852015cf8f004d1f7a01274b76cb033d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentence:\n",
      "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
      "\n",
      "Word counts:\n",
      "Counter({'mr': 1, 'brown': 1, 'jump': 1, 'lazy': 1, 'fox': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed sentence:')\n",
    "print(processed_sent2)\n",
    "print()\n",
    "print('Word counts:')\n",
    "print(Counter(processed_sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e51b4c01-2712-4ec9-852b-133716681b53",
    "_uuid": "25205be78a242dcb9b8bd653cd98244ce73ba69b"
   },
   "source": [
    "# Vectorization\n",
    "\n",
    "Let's put the words and counts into a nice table:\n",
    "\n",
    "| | brown | quick | fox | jump | lazy | dog | mr | \n",
    "|:---- |:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| Sent1 | 2 | 1 | 1 | 1 | 1 | 1 | 0 |  \n",
    "| Sent2 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | \n",
    "\n",
    "\n",
    "If we fix the positions of the vocabulary i.e. \n",
    "\n",
    "```\n",
    "[brown, quick, fox, jump, lazy, dog, mr]\n",
    "```\n",
    "\n",
    "and we do the counts for each word in each sentence, we get the sentence vectors (i.e. list of numbers to represent each sentence):\n",
    "\n",
    "```\n",
    "sent1 = [2,1,1,1,1,1,0]\n",
    "sent2 = [1,0,1,1,1,0,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64860ccd-4cf4-46dd-89d2-3929d9b7d539",
    "_uuid": "5c268c3e117c595b2835cdfd7c5abe604f35e0fa"
   },
   "source": [
    "# Vectorization with sklearn \n",
    "\n",
    "In `scikit-learn`, there're pre-built functions to do the preprocessing and vectorization that we've been doing using the `CountVectorizer` object. \n",
    "\n",
    "It will be the object that contains the vocabulary (i.e. the first row of our table above) and has the function to convert any sentence into the counts vectors we see as above.\n",
    "\n",
    "The input that `CountVectorizer` is a textfile, so we've to do some hacking to put let it accept the string outputs.\n",
    "\n",
    "We can \"fake it to make it\" using `io.StringIO` where we can convert any string to work like a file, e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "5f92d415-5016-4405-b797-d98df2393a89",
    "_uuid": "3834e62e0c3879b37f94e1924e80534a5899fbfd"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Create the vectorizer\n",
    "    count_vect = CountVectorizer()\n",
    "    count_vect.fit_transform(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "a8a7db77-9578-4f58-8c09-8cde03f6a865",
    "_uuid": "36407729e4ed2636089293545a5f2d030a4d60e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 8,\n",
       " 'quick': 7,\n",
       " 'brown': 0,\n",
       " 'fox': 2,\n",
       " 'jumps': 3,\n",
       " 'over': 6,\n",
       " 'lazy': 4,\n",
       " 'dog': 1,\n",
       " 'mr': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the vocabulary in our vectorizer\n",
    "# It's a dictionary where the words are the keys and \n",
    "# The values are the IDs given to each word. \n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13d2b03b-6ab9-4da4-b20e-38c3cce08243",
    "_uuid": "e2d864d079623c6d92e2694dad75bd8edf810d48"
   },
   "source": [
    "**Note:** We haven't counted anything yet just initializing our vectorizer object with the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2249db3e-0b22-4f63-be8e-b19c951a4ea2",
    "_uuid": "07b29c69132ed41c5da7f869f098694374b42cda"
   },
   "source": [
    "#  (Wait a minute)\n",
    "\n",
    "I didn't tell the vectorizer to remove punctuation and tokenize and lowercase, how did they do it?\n",
    "\n",
    "Also, `the` is in the vocabulary, it's a stopword, we want it gone... <br>\n",
    "And `jumps` isn't stemmed or lemmatized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "035f1601-9e72-4d85-929e-44d8447d7253",
    "_uuid": "e6393e0f95e0b0dbd92f64f22883331f9eaac919"
   },
   "source": [
    "If we look at the documentation of the [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in `sklearn`, we see:\n",
    "\n",
    "\n",
    "```python\n",
    "CountVectorizer(\n",
    "    input=’content’, encoding=’utf-8’, \n",
    "    decode_error=’strict’, strip_accents=None, \n",
    "    lowercase=True, preprocessor=None, \n",
    "    tokenizer=None, stop_words=None, \n",
    "    token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), \n",
    "    analyzer=’word’, max_df=1.0, min_df=1, \n",
    "    max_features=None, vocabulary=None, \n",
    "    binary=False, dtype=<class ‘numpy.int64’>)[source]\n",
    "```\n",
    "\n",
    "And more specifically:\n",
    "\n",
    "> **analyzer** : string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
    "> \n",
    "> Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "> If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "\n",
    " \n",
    "> **preprocessor** : callable or None (default)\n",
    "> \n",
    "> Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "> **tokenizer** : callable or None (default)\n",
    "> \n",
    "> Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word'.\n",
    "\n",
    "> **stop_words** : string {‘english’}, list, or None (default)\n",
    "> \n",
    "> If ‘english’, a built-in stop word list for English is used.\n",
    "> If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "If None, no stop words will be used. \n",
    "\n",
    "> **lowercase** : boolean, True by default\n",
    "> \n",
    "> Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9eb50e4d-4d52-47a4-9607-fd33fe3775d1",
    "_uuid": "11d7d8b6406c5d56259d72c48f97747e119885b7"
   },
   "source": [
    "# Achso, we can override these arguments with the functions we have learnt before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9c00de2-ab64-4a35-a790-485a64b43dd3",
    "_uuid": "4875ebac4a09758236313d7ffa17fb015faac926"
   },
   "source": [
    "We can **override the tokenizer and stop_words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "25630055-8410-46fe-b08c-5fc7202bf1ab",
    "_uuid": "f91435b9a2d476d45d9146ceb204f64e586133c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '``', 'ai', 'ca', \"n't\", 'sha', 'wo'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(stop_words=stoplist_combined,\n",
    "                                 tokenizer=word_tokenize)\n",
    "    count_vect.fit_transform(fin)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f29251e-1074-4c04-a50c-d56d4ca3a97e",
    "_uuid": "e23d630dd87abeae6ed1b37d64c2fe5223da7088"
   },
   "source": [
    "Or just **override the analyzer** totally with our preprocess text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "40813b12-f701-4bca-a2e3-c16b928dd32e",
    "_uuid": "fd10c8df7c870641807211b97140900e570c46af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 6, 'brown': 0, 'fox': 2, 'jump': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "    count_vect.fit_transform(fin)\n",
    "count_vect.vocabulary_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ab69e27d-1d13-47f0-a547-9e234292d166",
    "_uuid": "4024585e83698a0c454b6ff332de4a32d8bfeb8b"
   },
   "source": [
    "# To vectorize any new sentences, we use  `CountVectorizer.transform()` \n",
    "\n",
    "The function  will return a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "92d41c12-7f75-406e-80ec-66a2977a79a8",
    "_uuid": "e18dbaa0882825ffc4b1b746de13eb4d8381984d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([sent1, sent2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "81e138ab-0b0f-4c67-abe9-b34c15cb7e8d",
    "_uuid": "8815c6acca7b396407004eaf858a957576c06673"
   },
   "source": [
    "# To view the matrix, you can output it to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "4e4029f4-1440-4ebd-b239-c365e735e21d",
    "_uuid": "ec34de51827ed00a2cdb39c000a03bb85e582713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
      "\n",
      "Vocab: ('brown', 'dog', 'fox', 'jump', 'lazy', 'mr', 'quick')\n",
      "\n",
      "Matrix/Vectors:\n",
      " [[2 1 1 1 1 0 1]\n",
      " [1 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Print the words sorted by their index\n",
    "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
    "\n",
    "print(preprocess_text(sent1))\n",
    "print(preprocess_text(sent2))\n",
    "print()\n",
    "print('Vocab:', words_sorted_by_index)\n",
    "print()\n",
    "print('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c520f436-9d65-4950-a7aa-91eaf9382d12",
    "_uuid": "0c271bfc242864673a862c0fc4e587ad57654528"
   },
   "source": [
    "Naive Bayes \n",
    "====\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Classification\n",
    "====\n",
    "\n",
    "Classification simply means putting our data points into bins/box. You can also think of it as assigning label to our data points, e.g. given box of fruits, sort them in apples, oranges and others. \n",
    "\n",
    "Okay, the explanation could be more complex than that but `import this` says:\n",
    "\n",
    "> **Simple is better than complex.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1fd3db77-b764-4f26-914f-ec0ac4e4d0c9",
    "_uuid": "5d163c46b118b580f07eeebad0a16eaf5edbb20d"
   },
   "source": [
    "# Now that we learnt some basic NLP and vectorization, lets apply it to a fun task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a6d1a71-4171-4384-a0b8-5cae9efbea33",
    "_uuid": "26ce98337b4e3c9f54278412e578117bfd92d625"
   },
   "source": [
    "[Random Acts of Pizza](https://www.kaggle.com/c/random-acts-of-pizza)\n",
    "=====\n",
    "\n",
    "In machine learning, it is often said there are [no free lunches](). How wrong we were.\n",
    "\n",
    "This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. \n",
    "\n",
    "![](https://kaggle2.blob.core.windows.net/competitions/kaggle/3949/media/pizzas.png)\n",
    "\n",
    "The task is to create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c85876b-1b19-4c50-8330-a2c9dbd91f32",
    "_uuid": "3cae972238580fe9c4fb6a184b3661b232f77458"
   },
   "source": [
    "# Lets take a look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "211f0cf5-8be5-482e-bc3c-d57fd089689c",
    "_uuid": "efab58829e4f5082cd9ac154953f73c1f5033763"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./random_acts_of_pizza/train.json') as fin:\n",
    "    trainjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "0bb18893-12ed-4178-89cc-06c4f01d7107",
    "_uuid": "f363473d1aaaedd58eae40f7c970d1eedf6c7fa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'number_of_downvotes_of_request_at_retrieval': 0,\n",
       " 'number_of_upvotes_of_request_at_retrieval': 1,\n",
       " 'post_was_edited': False,\n",
       " 'request_id': 't3_l25d7',\n",
       " 'request_number_of_comments_at_retrieval': 0,\n",
       " 'request_text': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_text_edit_aware': 'Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated',\n",
       " 'request_title': 'Request Colorado Springs Help Us Please',\n",
       " 'requester_account_age_in_days_at_request': 0.0,\n",
       " 'requester_account_age_in_days_at_retrieval': 792.4204050925925,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_days_since_first_post_on_raop_at_retrieval': 792.4204050925925,\n",
       " 'requester_number_of_comments_at_request': 0,\n",
       " 'requester_number_of_comments_at_retrieval': 0,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_comments_in_raop_at_retrieval': 0,\n",
       " 'requester_number_of_posts_at_request': 0,\n",
       " 'requester_number_of_posts_at_retrieval': 1,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_posts_on_raop_at_retrieval': 1,\n",
       " 'requester_number_of_subreddits_at_request': 0,\n",
       " 'requester_received_pizza': False,\n",
       " 'requester_subreddits_at_request': [],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_minus_downvotes_at_retrieval': 1,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 0,\n",
       " 'requester_upvotes_plus_downvotes_at_retrieval': 1,\n",
       " 'requester_user_flair': None,\n",
       " 'requester_username': 'nickylvst',\n",
       " 'unix_timestamp_of_request': 1317852607.0,\n",
       " 'unix_timestamp_of_request_utc': 1317849007.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainjson[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24d5a591-79a6-4740-8b58-c76fef459355",
    "_uuid": "d231a3cdad2807893f44de861892093d02cb07a7"
   },
   "source": [
    "We're only interested in the text fields:\n",
    "\n",
    "**Input**:\n",
    " - `request_id`: unique identifier for the request \n",
    " - `request_title`: title of the reddit post for pizza request\n",
    " - `request_text_edit_aware`: expository to request for pizza\n",
    " \n",
    "**Output**:\n",
    " - `requester_recieved_pizza`: whether requester gets his/her pizza\n",
    " \n",
    "For our purpose, lets only use the `request_text` as the input to build our Naive Bayes classifier and the output is the `requester_recieved_pizza` field.\n",
    "\n",
    "**Note:** The `request_id` is only used for mapping purpose when we're submitting the results to the Kaggle task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "38df138d-858e-4983-be85-37f864b8b7aa",
    "_uuid": "9755249c3af76081f4203b75f2fbe05e860db4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID:\t t3_l25d7 \n",
      "\n",
      "Title:\t Request Colorado Springs Help Us Please \n",
      "\n",
      "Text:\t Hi I am in need of food for my 4 children we are a military family that has really hit hard times and we have exahusted all means of help just to be able to feed my family and make it through another night is all i ask i know our blessing is coming so whatever u can find in your heart to give is greatly appreciated \n",
      "\n",
      "Tag:\t False\n"
     ]
    }
   ],
   "source": [
    "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2d3b4d73-784e-4aef-a1c2-d70a0592495e",
    "_uuid": "bf75dd208d746cb254d08576477c25c2472d8cae"
   },
   "source": [
    "# Here's a neat trick to convert json to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "afae23a2-d175-4fd8-b63e-71a34284e500",
    "_uuid": "a4d14ae8096e83aea7ec4fe276799632f4a244a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_l25d7</td>\n",
       "      <td>Request Colorado Springs Help Us Please</td>\n",
       "      <td>Hi I am in need of food for my 4 children we a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_rcb83</td>\n",
       "      <td>[Request] California, No cash and I could use ...</td>\n",
       "      <td>I spent the last money I had on gas today. Im ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lpu5j</td>\n",
       "      <td>[Request] Hungry couple in Dundee, Scotland wo...</td>\n",
       "      <td>My girlfriend decided it would be a good idea ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_mxvj3</td>\n",
       "      <td>[Request] In Canada (Ontario), just got home f...</td>\n",
       "      <td>It's cold, I'n hungry, and to be completely ho...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_1i6486</td>\n",
       "      <td>[Request] Old friend coming to visit. Would LO...</td>\n",
       "      <td>hey guys:\\n I love this sub. I think it's grea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_l25d7            Request Colorado Springs Help Us Please   \n",
       "1   t3_rcb83  [Request] California, No cash and I could use ...   \n",
       "2   t3_lpu5j  [Request] Hungry couple in Dundee, Scotland wo...   \n",
       "3   t3_mxvj3  [Request] In Canada (Ontario), just got home f...   \n",
       "4  t3_1i6486  [Request] Old friend coming to visit. Would LO...   \n",
       "\n",
       "                             request_text_edit_aware  requester_received_pizza  \n",
       "0  Hi I am in need of food for my 4 children we a...                     False  \n",
       "1  I spent the last money I had on gas today. Im ...                     False  \n",
       "2  My girlfriend decided it would be a good idea ...                     False  \n",
       "3  It's cold, I'n hungry, and to be completely ho...                     False  \n",
       "4  hey guys:\\n I love this sub. I think it's grea...                     False  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(trainjson) # Pandas magic... \n",
    "df_train = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware', \n",
    "               'requester_received_pizza']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0c50ef1b-d0f5-480e-81cd-ae2922211564",
    "_uuid": "940b7bc059fb428bd08e6f1930b74f4e6a4d4823"
   },
   "source": [
    "# Lets take a look at the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "28c0923b-0ea3-456e-8311-3773a169684c",
    "_uuid": "d7edfb13cfc94358fdb89455626d7b8d68adf7d9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./random_acts_of_pizza/test.json') as fin:\n",
    "    testjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'giver_username_if_known': 'N/A',\n",
       " 'request_id': 't3_i8iy4',\n",
       " 'request_text_edit_aware': \"Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance!\",\n",
       " 'request_title': '[request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado',\n",
       " 'requester_account_age_in_days_at_request': 42.08386574074074,\n",
       " 'requester_days_since_first_post_on_raop_at_request': 0.0,\n",
       " 'requester_number_of_comments_at_request': 57,\n",
       " 'requester_number_of_comments_in_raop_at_request': 0,\n",
       " 'requester_number_of_posts_at_request': 10,\n",
       " 'requester_number_of_posts_on_raop_at_request': 0,\n",
       " 'requester_number_of_subreddits_at_request': 16,\n",
       " 'requester_subreddits_at_request': ['AskReddit',\n",
       "  'COents',\n",
       "  'Denver',\n",
       "  'DenverBroncos',\n",
       "  'LibraryofBabel',\n",
       "  'adventuretime',\n",
       "  'denvernuggets',\n",
       "  'fffffffuuuuuuuuuuuu',\n",
       "  'gaming',\n",
       "  'pics',\n",
       "  'techsupport',\n",
       "  'todayilearned',\n",
       "  'trees',\n",
       "  'videos',\n",
       "  'woahdude',\n",
       "  'worldnews'],\n",
       " 'requester_upvotes_minus_downvotes_at_request': 364,\n",
       " 'requester_upvotes_plus_downvotes_at_request': 840,\n",
       " 'requester_username': 'j_like',\n",
       " 'unix_timestamp_of_request': 1308963419.0,\n",
       " 'unix_timestamp_of_request_utc': 1308959819.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testjson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "775eec1e-2d63-4dd9-8f79-e11ae53a04e5",
    "_uuid": "e3a07d7af63044bd190dbe630de39fccfca61740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID:\t t3_i8iy4 \n",
      "\n",
      "Title:\t [request] pregger gf 95 degree house and no food.. promise to pay it forward! Northern Colorado \n",
      "\n",
      "Text:\t Hey all! It's about 95 degrees here and our kitchen is pretty much empty save for some bread and cereal.  My girlfriend/fiance is 8 1/2 months pregnant and we could use a good meal.  We promise to pay it forward when we get money! Thanks so much in advance! \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'requester_received_pizza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9cc2a9169db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Title:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Text:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'request_text_edit_aware'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tag:\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'requester_received_pizza'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'requester_received_pizza'"
     ]
    }
   ],
   "source": [
    "print('UID:\\t', testjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', testjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', testjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', testjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6cb14f9e-0c8e-40d3-aff7-e542790336eb",
    "_uuid": "df6e837ff69fbab2af2f47acffa0cb8d12f995a9"
   },
   "source": [
    "# Gotcha again! \n",
    "\n",
    "In the test data, our label (i.e. `requester_received_pizza`) **won't be known** to us since that's the thing that our classifier is predicting.\n",
    "\n",
    "**Note:** Whatever features that we're going to train our classifier with, we should have them in our test set too. In our case we need to make sure that the test set has `request_text_edit_aware` field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e4c5394-7c84-47e5-b859-9a60ff97f851",
    "_uuid": "aed441bb12db2d224cbea93c835ae1d1d27e363d"
   },
   "source": [
    "# Lets put the test data into a pandas DataFrame too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "d098e108-b1b6-4914-971a-114280869002",
    "_uuid": "830d4b563f3fea20101083bf3cb660f405249cc1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>request_title</th>\n",
       "      <th>request_text_edit_aware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>[request] pregger gf 95 degree house and no fo...</td>\n",
       "      <td>Hey all! It's about 95 degrees here and our ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>[Request] Lost my job day after labour day, st...</td>\n",
       "      <td>I didn't know a place like this exists! \\n\\nI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>(Request) pizza for my kids please?</td>\n",
       "      <td>Hi Reddit. Im a single dad having a really rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>[Request] Just moved to a new state(Waltham MA...</td>\n",
       "      <td>Hi I just moved to Waltham MA from my home sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>[Request] Two girls in between paychecks, we'v...</td>\n",
       "      <td>We're just sitting here near indianapolis on o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id                                      request_title  \\\n",
       "0   t3_i8iy4  [request] pregger gf 95 degree house and no fo...   \n",
       "1  t3_1mfqi0  [Request] Lost my job day after labour day, st...   \n",
       "2   t3_lclka                (Request) pizza for my kids please?   \n",
       "3  t3_1jdgdj  [Request] Just moved to a new state(Waltham MA...   \n",
       "4   t3_t2qt4  [Request] Two girls in between paychecks, we'v...   \n",
       "\n",
       "                             request_text_edit_aware  \n",
       "0  Hey all! It's about 95 degrees here and our ki...  \n",
       "1  I didn't know a place like this exists! \\n\\nI ...  \n",
       "2  Hi Reddit. Im a single dad having a really rou...  \n",
       "3  Hi I just moved to Waltham MA from my home sta...  \n",
       "4  We're just sitting here near indianapolis on o...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(testjson) # Pandas magic... \n",
    "df_test = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b869481f-c713-4aa6-aa0d-f2e5f4b2abfc",
    "_uuid": "a2cacb2270490a92ec9a7e920e8ca17c730c1e09"
   },
   "source": [
    "# Split training data before vectorization\n",
    "\n",
    "The first thing to do is to split our training data into 2 parts:\n",
    "\n",
    " - **training**: Use for training our model\n",
    " - **validation**: Use to check the \"soundness\" of our model\n",
    " \n",
    "**Note:** \n",
    "\n",
    " - Splitting the data into 2 parts and holding out one part to check the model is one of method to validate the \"soundness\" of our model. It's call the **hold-out** validation. \n",
    "\n",
    " - Another popular validation method is **cross-validation**, it's out of scope here but you can take a look at `crossvalidation` in `scikit-learn`. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "0b61f317-079b-4253-b94a-a87b71bd47fb",
    "_uuid": "ac1e8766dcf5d7d81fe4fd3a95e7bc659fbf1978"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# It doesn't really matter what the function name is called\n",
    "# but the `train_test_split` is splitting up the data into \n",
    "# 2 parts according to the `test_size` argument you've set.\n",
    "\n",
    "# When we're splitting up the training data, we're spltting up \n",
    "# into train, valid split. The function name is just a name =)\n",
    "train, valid = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8677912b-d9e2-4fd8-b722-acb3320af4fc",
    "_uuid": "f97576104bbabfc720fa0794ab6a8fc4e043f109"
   },
   "source": [
    "# Vectorize the train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "02efe9a8-00e4-41fa-9b6a-5fbebe751feb",
    "_uuid": "4549079bfedbbff679ccde5f083b3c6814e73233"
   },
   "outputs": [],
   "source": [
    "# Initialize the vectorizer and \n",
    "# override the analyzer totally with the preprocess_text().\n",
    "# Note: the vectorizer is just an 'empty' object now.\n",
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "# When we use `CounterVectorizer.fit_transform`,\n",
    "# we essentially create the dictionary and \n",
    "# vectorize our input text at the same time.\n",
    "train_set = count_vect.fit_transform(train['request_text_edit_aware'])\n",
    "train_tags = train['requester_received_pizza']\n",
    "\n",
    "# When vectorizing the validation data, we use `CountVectorizer.transform()`.\n",
    "valid_set = count_vect.transform(valid['request_text_edit_aware'])\n",
    "valid_tags = valid['requester_received_pizza']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85863807-de62-471c-a5f9-7ada4e54e0bf",
    "_uuid": "eb879f0ea99c654625d7e4526de4c1f5602619ac"
   },
   "source": [
    "# Now, we need to vectorize the test data too\n",
    "\n",
    "After we vectorize our data, the input to train the classifier would be the vectorized text. \n",
    "<br>When we predict the label with the trained mdoel, our input needs to be vectorized too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "766b1529-5967-45e9-9736-1fb69bdc6460",
    "_uuid": "502788a6150bc9c7256b097a4d5c04b4822ac3eb"
   },
   "outputs": [],
   "source": [
    "# When vectorizing the test data, we use `CountVectorizer.transform()`.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d7ae411-ae2c-4098-beb2-ac6c2dc267a9",
    "_uuid": "3aaf138395ccdfe7a29d89d1b58da9db96ae8b36"
   },
   "source": [
    "# Naive Bayes classifier in sklearn\n",
    "\n",
    "There are different variants of Naive Bayes (NB) classifier in `sklearn`. <br>\n",
    "For simplicity, lets just use the `MultinomialNB`.\n",
    "\n",
    "**Multinomial** is a big word but it just means many classes/categories/bins/boxes that needs to be classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "8a159538-c58a-485d-bc23-d7115ee55a77",
    "_uuid": "44cee633360c7989a3d67716553ef4e2bc9eae87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB() \n",
    "\n",
    "# To train the classifier, simple do \n",
    "clf.fit(train_set, train_tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c2776b5-6515-4321-a7e7-f925534241d6",
    "_uuid": "fbec702a6003287ea5d49049be532ee96b6d7a14"
   },
   "source": [
    "# Before we test our classifier on the test set, we get a sense of how good it is on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "a0bba339-cd62-46e9-9106-55ef15000dc5",
    "_uuid": "b5f7b1ac26a30b568d3f5dec2a3f76c34a316ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza reception accuracy = 73.14356435643565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# To predict our tags (i.e. whether requesters get their pizza), \n",
    "# we feed the vectorized `test_set` to .predict()\n",
    "predictions_valid = clf.predict(valid_set)\n",
    "\n",
    "print('Pizza reception accuracy = {}'.format(\n",
    "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24fed6a6-9b91-4192-92e1-1c6ac3264dab",
    "_uuid": "293f22791ce16ad0071afa625f7b6efa0601da7d"
   },
   "source": [
    "# Now lets use the full training data set and re-vectorize and retrain the classifier\n",
    "\n",
    "More data == better model (in most cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "f738aa5b-8e1b-4885-8f3e-6d279f23d082",
    "_uuid": "4a0c2bbe6b98000d71e1b2f9cd00bb01b156522e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "full_train_set = count_vect.fit_transform(df_train['request_text_edit_aware'])\n",
    "full_tags = df_train['requester_received_pizza']\n",
    "\n",
    "# Note: We have to re-vectorize the test set since\n",
    "#       now our vectorizer is different using the full \n",
    "#       training set.\n",
    "test_set = count_vect.transform(df_test['request_text_edit_aware'])\n",
    "\n",
    "# To train the classifier\n",
    "clf = MultinomialNB() \n",
    "clf.fit(full_train_set, full_tags) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6013d400-4a01-438a-9afe-b540b6ec78b9",
    "_uuid": "dbf9eb2a3dde29a26ae51df05f120ee26334d0c3"
   },
   "source": [
    "# Finally, we use the classifier to predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "2fe3ff2b-5157-43b2-ad8d-55389ad28a0e",
    "_uuid": "450827a713f7eaaa2d3c3504b038a513c27b3b20"
   },
   "outputs": [],
   "source": [
    "# To predict our tags (i.e. whether requesters get their pizza), \n",
    "# we feed the vectorized `test_set` to .predict()\n",
    "predictions = clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef6b8e68-46be-4944-b809-872ddeabf386",
    "_uuid": "51a804beaf030dbb8cbdfc320d6a26d86991d274"
   },
   "source": [
    "**Note:** Since we don't have the `requester_received_pizza` field in test data, we can't measure accuracy. But we can do some exploration as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ac9a825-8ffa-4ebd-a223-73521a27bc10",
    "_uuid": "e948c2b162d178cd674689adc1ce9dcdb426cbb5"
   },
   "source": [
    "# From the training data, we had 24% pizza giving rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "80300555-265f-4557-8b52-17c343a34c58",
    "_uuid": "4ad28c27f351de96d40b68aed92217b17f71c542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 4040 requests, only 994 gets their pizzas, 24.603960396039604% success rate...\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(df_train['requester_received_pizza']) / len(df_train) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "          ' {}% success rate...'.format(len(df_train), \n",
    "                                        sum(df_train['requester_received_pizza']), \n",
    "                                       success_rate)\n",
    "         )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3301d9e0-544f-41d5-9c32-b2ea95b37871",
    "_uuid": "9058b0225d2f7e87486ffb5e31f710c55431a39c"
   },
   "source": [
    "# Lolz, our classifier is rather stingy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "111c7d74-3516-455d-a13d-8a0af3d1b3c6",
    "_uuid": "b6b08ebc7396962d4268858d27e7f1f082943891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 1631 requests, only 51 gets their pizzas, 3.126916002452483% success rate...\n"
     ]
    }
   ],
   "source": [
    "success_rate = sum(predictions) / len(predictions) * 100\n",
    "print(str('Of {} requests, only {} gets their pizzas,'\n",
    "          ' {}% success rate...'.format(len(predictions), \n",
    "                                        sum(predictions), \n",
    "                                       success_rate)\n",
    "         )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8ed85498-5dd2-48bf-a379-1b84d237a42b",
    "_uuid": "8fbd1af6bdd2f1c3e1ff0cf65976fa3252e171cc"
   },
   "source": [
    "# How accurate is our count vectorization naive bayes classifier on the test data?\n",
    "\n",
    "Since we don't have the `requester_received_pizza` field in the test data, we have to check that with an oracle (i.e. the person that knows). \n",
    "\n",
    "On Kaggle, **checking with the oracle** means uploading the file in the correct format and their script will process the scores and tell you how you did.\n",
    "\n",
    "**Note:** Different tasks will use different metrics but in most cases getting as many correct predictions as possible is the thing to aim for. We won't get into the details of how classifiers are evaluated but for a start, please see [precision, recall and F1-scores](https://en.wikipedia.org/wiki/Precision_and_recall) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8054679-9d04-4563-91d5-daa29d0f7ce6",
    "_uuid": "9129b8f430bf2c652038a5a7316c0e9db1bb4f4a"
   },
   "source": [
    "# Finally, lets take a look at what format the oracle expects and create the output file for our predictions accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "a683db0e-da0f-48b0-915c-c3bc288519f9",
    "_uuid": "66603c8033bf346a9cd418d2679c695e1973a820"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id  requester_received_pizza\n",
       "0   t3_i8iy4                         0\n",
       "1  t3_1mfqi0                         0\n",
       "2   t3_lclka                         0\n",
       "3  t3_1jdgdj                         0\n",
       "4   t3_t2qt4                         0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_submission = pd.read_csv('./random_acts_of_pizza/sampleSubmission.csv')\n",
    "df_sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "f28c1ce2-864d-4de3-926e-4dc317479216",
    "_uuid": "1410d9b66337fc2924eb156277f059e116143919"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>requester_received_pizza</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_i8iy4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_1mfqi0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_lclka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_1jdgdj</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_t2qt4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  request_id  requester_received_pizza\n",
       "0   t3_i8iy4                         0\n",
       "1  t3_1mfqi0                         0\n",
       "2   t3_lclka                         0\n",
       "3  t3_1jdgdj                         0\n",
       "4   t3_t2qt4                         0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've kept the `request_id` previous in the `df_test` dataframe.\n",
    "# We can simply merge that column with our predictions.\n",
    "df_output = pd.DataFrame({'request_id': list(df_test['request_id']), \n",
    "                          'requester_received_pizza': list(predictions)}\n",
    "                        )\n",
    "# Convert the predictions from boolean to integer.\n",
    "df_output['requester_received_pizza'] = df_output['requester_received_pizza'].astype(int)\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "4053487b-e0d6-48a3-ba95-dbe2465f78f7",
    "_uuid": "e64b0d64cbea2a60b878ca0e445b27c722e368db",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the csv file.\n",
    "df_output.to_csv('basic-nlp-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "a97cb294-eb6c-479e-944a-b82e63dc008f",
    "_uuid": "dbbd128b7754fa488986036a55ce09ecbe52e167",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
