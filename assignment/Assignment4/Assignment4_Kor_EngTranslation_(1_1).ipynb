{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "snu36",
      "language": "python",
      "name": "snu36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Assignment4_Kor_EngTranslation (1.1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8yTr1wPPtJ9W"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb_ue49GtJ9J"
      },
      "source": [
        "# Final Project - Korean to English Translation\n",
        "\n",
        "- Sequence to Sequence 모델의 대표적인 한국어-영어 번역을 Encoder-decoder, Attention, Convolution, 그리고 Transformers 기반으로 구현\n",
        "- 수업시간에 살펴본 Pytorch Seq to Seq 모델 (https://github.com/bentrevett/pytorch-seq2seq)을 참조로 하여 한국어와 영어 형태소분석되고 의존관계로 되어 있는 파일을 프로세싱하여 두 언어의 parallel 데이터 쌍으로 만들고 이를 학습하여 모델별로 Perplexity가 어떻게 달라지는지 살펴 보고, 가장 성능이 좋은 모델을 근간으로 해서 Inference로 한국어 문장을 입력하면 대응되는 영어 번역이 출력될 수 있도록 구현\n",
        "- 반드시 다음 세 모델에 대해서 PPL와 BLEU score가  다 체크되어야 함. (Packed) Encoder-Decoder, Convolutional Seq to Seq, Transformers ...\n",
        "- 세 모델 중에 학습이 제대로 이루어지지 않는 경우, PPL이나 BLEU가 문제가 있는 경우 이를 Fix하려고 시도해 보라.\n",
        "- Inference시에 unk인 단어를 로마자화해서 번역에 나타날 수 있도록 시도해 볼 것(참고할 수 있는 사이트 중 하나 https://github.com/osori/korean-romanizer)\n",
        "\n",
        "- 개인적으로 하거나 최대 두명까지 그룹 허용. \n",
        "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 Assignment4_[DS또는 CL]_학과_이름.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-vJK94ktJ9O"
      },
      "source": [
        "## Data 1\n",
        "- 첨부된 ko-en-en.parse.syn은 330,974 한국어 문장에 대응되는 영어문장이 품사와 구문분석이 되어 있는 파일이고 ko-en-ko.parse.syn은 이에 대응되는 한국어 문장이 형태소와 구문분석이 되어 있는 파일이다.\n",
        "\n",
        "(ROOT (S (NP (NNP Flight) (NNP 007)) (VP (MD will) (VP (VB stay) (PP (IN on) (NP (NP (DT the) (NN ground)) (PP (IN for) (NP (CD one) (NN hour))))))) (. .)))\n",
        "\n",
        "\n",
        "<id 1>\n",
        "<sent 1>\n",
        "1       2       NP      777/SN\n",
        "2       6       NP_SBJ  항공편/NNG|은/JX\n",
        "3       4       NP      1/SN|시간/NNG\n",
        "4       6       NP_AJT  동안/NNG\n",
        "5       6       NP_AJT  지상/NNG|에/JKB\n",
        "6       7       VP      머물/VV|게/EC\n",
        "7       0       VP      되/VV|ㅂ니다/EF|./SF\n",
        "</sent>\n",
        "</id>\n",
        "\n",
        "    - 이 두 파일을 프로세싱하여 한-영 병렬 데이터로 만들고 이를 학습 및 테스트 데이터로 사용한다.\n",
        "    - Hint: 구조화된 데이터를 프로세싱하기 위해서는 nltk의 모듈을 사용할 수 있다.\n",
        "\n",
        "    - 한국어 형태소 분석된 단위를 어절별로 결합할 수 있고, 분석된 채로 그대로 사용할 수도 있다.\n",
        "    - 두 언어의 어순을 비슷하게 데이터를 만들어 학습할 수도 있고, 번역의 성능을 높이기 위해 다양한 형태로 재구조화 할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcvx3hhNtJ9P"
      },
      "source": [
        "## Data 2\n",
        "[Korean Parallel Corpora](https://github.com/jungyeul/korean-parallel-corpora)에는 Korean-English-jhe, Korean-English-news-v1의 병렬 데이터가 있다. \n",
        "- 이 데이터를 다운 받아 Data1의 자료와 합쳐서 사용할 수 있다\n",
        "- 이 경우 형태소 분석이 된 경우와 그렇지 않은 자료가 있으니, Data1의 자료와 합칠 때 형태소 분석을 하거나 아니면 어절 단위로 결합하여 할 수도 있다.\n",
        "- 전체적인 일관성 및 inference를 위해서 형태소 분석된 것이 더 좋을 수 있는데, 이 경우 형태소 분석되지 않는 데이터는 새로 형태소 분석을 할 필요. 단 형태소 분석 단위가 일치하는지 알아볼 필요. 형태소 분석 단위가 완전히 일치하지 않는다면 가급적 분석단위 일치도가 높은 형태소 분석기를 선택할 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EqgXqZStJ9P"
      },
      "source": [
        "## 구현,실험 전체적인 설명 및 분석 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSwD-NMutJ9Q"
      },
      "source": [
        "## Your Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywk2ZOmltJ9Q",
        "outputId": "b4d14fd0-9b8b-4fe9-8891-cf80588bc8d2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "import json # save parsed list\n",
        "import gensim #!# I used gensim==3.8.1 instead of latest version, since it has some error\n",
        "from nltk import Tree\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from sys import platform\n",
        "if 'linux' == platform:\n",
        "  print('Jeehun Operating code')\n",
        "  from google.colab import drive ; drive.mount('/content/drive') # Colab setting\n",
        "  !pip install einops # Colab setting\n",
        "  os.chdir('/content/drive/MyDrive/[NLP2021] Ass4')\n",
        "\n",
        "\n",
        "  from einops import rearrange, reduce"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jeehun Operating codew\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzqsgR7ZtJ9R"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnm6Z8U5tJ9S",
        "outputId": "a29ae3f9-d3fa-441c-cbc3-ca974c90a3a3"
      },
      "source": [
        "with open('ko-en.ko.parse','r') as f:\n",
        "    ko = list(f)\n",
        "\n",
        "## split 으로 구분할 수 있는 형태로 전처리\n",
        "# 문장 단위로 분리하기\n",
        "# 문장 내부에서 token 추출하기\n",
        "\n",
        "def extract_sentence(token_list):\n",
        "    '''extract <sent #> sth </sent>'''\n",
        "    sentence_list = []\n",
        "    sentence = ''\n",
        "    in_sentence = False\n",
        "\n",
        "    for tok in token_list:\n",
        "        if '<id' in tok:\n",
        "            in_sentence = True\n",
        "\n",
        "        if in_sentence:\n",
        "            sentence += tok\n",
        "\n",
        "        if '</id>' in tok:\n",
        "            sentence_list.append(sentence)\n",
        "            in_sentence = False\n",
        "            sentence = ''\n",
        "            \n",
        "    return sentence_list\n",
        "\n",
        "def extract_token(segment_with_morpheme) -> list:\n",
        "    '''split token/morpheme|...|token/morpheme'''\n",
        "    '''since we tokenize the data before import on torchtext, convert every string object to lowercase'''\n",
        "    token_list = []\n",
        "    if '|' in segment_with_morpheme: # multiple token in a sement\n",
        "        for token in segment_with_morpheme.split(sep = '|'):\n",
        "            token_list.append(str.lower(token.split(sep = '/')[0]))\n",
        "        \n",
        "    else: # multiple token in a sement\n",
        "        token_list.append(str.lower(segment_with_morpheme.split(sep = '/')[0]))\n",
        "        \n",
        "    return token_list\n",
        "\n",
        "def preprocess(ko):\n",
        "    print('>>>>>>> [1/3] Initailize', len(ko))\n",
        "    processed_sentence_list = []\n",
        "    sentence_list = extract_sentence(ko)\n",
        "    print('>>>>>>> [2/3] Extract sentence', len(sentence_list))\n",
        "\n",
        "\n",
        "    print('>>>>>>> [3/3] Final', len(processed_sentence_list))\n",
        "    return processed_sentence_list\n",
        "\n",
        "processed_ko = preprocess(ko)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330974/330974 [00:05<00:00, 60378.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjQReQcytJ9T",
        "outputId": "828800f4-1ff8-4705-90d2-ca3f7e4c0dc2"
      },
      "source": [
        "# English data\n",
        "with open('ko-en.en.parse.syn') as f:\n",
        "    en = f.readlines()\n",
        "\n",
        "processed_en = []\n",
        "for idx, _ in tqdm(enumerate(en)):\n",
        "    t = Tree.fromstring(en[idx])\n",
        "    processed_token_list = [str.lower(token) for token in t.leaves()]\n",
        "    \n",
        "    processed_token_list.insert(0, '<sos>')\n",
        "    processed_token_list.append('<eos>')\n",
        "    \n",
        "    processed_en.append(processed_token_list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "330974it [00:23, 13830.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkMntDRAtJ9U"
      },
      "source": [
        "## torchtext 에 전달하기\n",
        "# save parsed dataset as a json file\n",
        "split_idx = len(processed_ko) // 10\n",
        "\n",
        "ko_en_json_train = [{'ko' : ko, 'en' : en} for ko, en in zip(processed_ko[:split_idx*8], processed_en[:split_idx*8])] #!# change it after practice\n",
        "ko_en_json_valid = [{'ko' : ko, 'en' : en} for ko, en in zip(processed_ko[split_idx*8:split_idx*9], processed_en[split_idx*8:split_idx*9])] #!# change it after practice\n",
        "ko_en_json_test  = [{'ko' : ko, 'en' : en} for ko, en in zip(processed_ko[split_idx*9:], processed_en[split_idx*9:])] #!# change it after practice\n",
        "\n",
        "ko_en_json_small = [{'ko' : ko, 'en' : en} for ko, en in zip(processed_ko[:256*8], processed_en[:256*8])] #!# for debug\n",
        "\n",
        "with open(\"ko_en_parsed_train.json\" , encoding= \"utf-8\", mode=\"w\") as file: \n",
        "    for i in ko_en_json_train: file.write(json.dumps(i) + \"\\n\")\n",
        "        \n",
        "with open(\"ko_en_parsed_valid.json\" , encoding= \"utf-8\", mode=\"w\") as file: \n",
        "    for i in ko_en_json_valid: file.write(json.dumps(i) + \"\\n\")\n",
        "        \n",
        "with open(\"ko_en_parsed_test.json\" , encoding= \"utf-8\", mode=\"w\") as file: \n",
        "    for i in ko_en_json_test: file.write(json.dumps(i) + \"\\n\")\n",
        "        \n",
        "with open(\"ko_en_parsed_small.json\" , encoding= \"utf-8\", mode=\"w\") as file: \n",
        "    for i in ko_en_json_small: file.write(json.dumps(i) + \"\\n\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLQLXgX3-VsF"
      },
      "source": [
        "os.chdir('/content/drive/My Drive/[NLP2021] Ass4')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdA2VtrbtJ9V",
        "outputId": "efd11d02-4f24-49f8-ca31-b3a2633bc400"
      },
      "source": [
        "## pack padded seq2seq\n",
        "# torchtext iterator 만들어주기\n",
        "KOREAN = data.Field( \n",
        "    init_token = '<sos>', \n",
        "    eos_token = '<eos>', \n",
        "    include_lengths = True\n",
        ")\n",
        "\n",
        "ENGLISH = data.Field( \n",
        "    init_token = '<sos>', \n",
        "    eos_token = '<eos>'\n",
        ")\n",
        "\n",
        "fields  = {'ko' : ('ko', KOREAN), 'en' : ('en', ENGLISH)}\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                            path  = os.getcwd(), #!#\n",
        "                            train = \"ko_en_parsed_small.json\", #!# 'ko_en_parsed_train.json'\n",
        "                            test  = \"ko_en_parsed_small.json\", #!# 'ko_en_parsed_test.json'\n",
        "                            format = 'json',\n",
        "                            fields = fields\n",
        ")\n",
        "\n",
        "print(vars(train_data.examples[63])['en'])\n",
        "print(vars(train_data.examples[63])['ko'])\n",
        "\n",
        "BATCH_SIZE = 256 #!# hyper parameter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "     (train_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.ko),\n",
        "     device = device)\n",
        "\n",
        "KOREAN.build_vocab(train_data, max_size=10000, min_freq=2) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
        "\n",
        "ENGLISH.build_vocab(train_data, max_size=10000, min_freq=2) # Word2Vec 모델을 임베딩 벡터값으로 초기화"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', '10', 'minutes', '.', 'never', 'mind', '.', '<eos>']\n",
            "['<sos>', '10', '분', '이', 'ㅂ니다', '.', '신경', '쓰', '지', '말', '시', '어요', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54MDRGMctJ9W",
        "outputId": "f805ca45-7d26-4224-dd52-4ef8525c6089"
      },
      "source": [
        "## Conv. seq2seq, Transfomer seq2seq\n",
        "# torchtext iterator 만들어주기\n",
        "CNN_KOREAN = data.Field( \n",
        "#     sequential = False, #!#\n",
        "    init_token = '<sos>', \n",
        "    eos_token = '<eos>', \n",
        "    batch_first = True,\n",
        "    include_lengths = False #!#\n",
        ")\n",
        "\n",
        "CNN_ENGLISH = data.Field( \n",
        "    init_token = '<sos>', \n",
        "    eos_token = '<eos>', \n",
        "    batch_first = True\n",
        ")\n",
        "\n",
        "CNN_fields  = {'ko' : ('ko', CNN_KOREAN), 'en' : ('en', CNN_ENGLISH)}\n",
        "\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                            path  = os.getcwd(), #!#\n",
        "                            train = \"ko_en_parsed_small.json\", #!# 'ko_en_parsed_train.json'\n",
        "                            test  = \"ko_en_parsed_small.json\", #!# 'ko_en_parsed_test.json'\n",
        "                            format = 'json',\n",
        "                            fields = CNN_fields\n",
        ")\n",
        "\n",
        "print(vars(train_data.examples[63])['en'])\n",
        "print(vars(test_data.examples[63])['ko'])\n",
        "\n",
        "BATCH_SIZE = 64 #!# hyper parameter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "CNN_train_iterator, CNN_test_iterator = data.BucketIterator.splits(\n",
        "     (train_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.ko),\n",
        "     device = device)\n",
        "\n",
        "CNN_KOREAN.build_vocab(train_data, max_size=10000, min_freq=2) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
        "\n",
        "CNN_ENGLISH.build_vocab(train_data, max_size=10000, min_freq=2) # Word2Vec 모델을 임베딩 벡터값으로 초기화"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<sos>', '10', 'minutes', '.', 'never', 'mind', '.', '<eos>']\n",
            "['<sos>', '10', '분', '이', 'ㅂ니다', '.', '신경', '쓰', '지', '말', '시', '어요', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yTr1wPPtJ9W"
      },
      "source": [
        "## use pre-trained w2v\n",
        "### Before you run the code, make sure to download w2v file from the link below. (or just ignore them)\n",
        "- ko : https://drive.google.com/open?id=0B0ZXk88koS2KbDhXdWg1Q2RydlU\n",
        "- en : http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JjSBsmAD5Wx3",
        "outputId": "1e1d49b0-b28c-4cb8-bd4d-67c9e8ccef39"
      },
      "source": [
        "# !pip uninstall fasttext\n",
        "# import fastText as fasttext\n",
        "import fasttext\n",
        "pretrained = 'cc.ko.300.bin'\n",
        "ft = fasttext.load_model(os.path.join(os.getcwd(), pretrained))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-971dbac06801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cc.ko.300.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0meprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: /content/drive/MyDrive/[NLP2021] Ass4/fastText-0.2.0/cc.ko.300.bin cannot be opened for loading!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTWUQcRHtJ9W"
      },
      "source": [
        "## 사전 학습된 word2vec 가져오기\n",
        "# code for using Korean w2v\n",
        "# 모델 불러오고 저장하기\n",
        "PATH = os.getcwd()\n",
        "ko_w2v = gensim.models.Word2Vec.load(PATH + 'ko.bin') # before run the code, make sure you download ko.bin from https://drive.google.com/open?id=0B0ZXk88koS2KbDhXdWg1Q2RydlU\n",
        "ko_w2v.wv.save_word2vec_format(PATH + 'ko.bin')\n",
        "\n",
        "# 저장한 w2v 모델 불러오고 torchtext.vocab 에 적용하기\n",
        "PATH = '/home/enkeejunior1/study_NLP/assignment/Assignment4/'\n",
        "ko_vectors = Vectors(name = 'ko.bin', cache = PATH) # model_name + path = path_to_embeddings_file\n",
        "KOREAN.build_vocab(train_data, max_size=10000, min_freq=2,\n",
        "                   vectors = ko_vectors) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
        "\n",
        "CNN_KOREAN.build_vocab(train_data, max_size=10000, min_freq=2,\n",
        "                   vectors = ko_vectors) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
        "\n",
        "#!# 실제 embedding 결과 확인\n",
        "\n",
        "# embedding layer 에 적용하기\n",
        "ko_embedding_layer = nn.Embedding.from_pretrained(KOREAN.vocab.vectors, freeze = False)\n",
        "CNN_ko_embedding_layer = nn.Embedding.from_pretrained(CNN_KOREAN.vocab.vectors, freeze = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjz3uWPNtJ9X"
      },
      "source": [
        "# code for using English w2v\n",
        "# https://stackoverflow.com/questions/51323344/cant-load-glove-6b-300d-txt\n",
        "# import gensim\n",
        "# import os\n",
        "# import shutil\n",
        "# import hashlib\n",
        "# from sys import platform\n",
        "\n",
        "# def getFileLineNums(filename):\n",
        "#     f = open(filename, 'r')\n",
        "#     count = 0\n",
        "#     for line in f:\n",
        "#         count += 1\n",
        "#     return count\n",
        "\n",
        "\n",
        "# def prepend_line(infile, outfile, line):\n",
        "#     with open(infile, 'r') as old:\n",
        "#         with open(outfile, 'w') as new:\n",
        "#             new.write(str(line) + \"\\n\")\n",
        "#             shutil.copyfileobj(old, new)\n",
        "\n",
        "# def prepend_slow(infile, outfile, line):\n",
        "#     with open(infile, 'r') as fin:\n",
        "#         with open(outfile, 'w') as fout:\n",
        "#             fout.write(line + \"\\n\")\n",
        "#             for line in fin:\n",
        "#                 fout.write(line)\n",
        "\n",
        "# def load(filename):\n",
        "#     num_lines = getFileLineNums(filename)\n",
        "#     gensim_file = 'glove_model.txt'\n",
        "#     gensim_first_line = \"{} {}\".format(num_lines, 300)\n",
        "#     # Prepends the line.\n",
        "#     if platform == \"linux\" or platform == \"linux2\":\n",
        "#         prepend_line(filename, gensim_file, gensim_first_line)\n",
        "#     else:\n",
        "#         prepend_slow(filename, gensim_file, gensim_first_line)\n",
        "\n",
        "#     model = gensim.models.KeyedVectors.load_word2vec_format(gensim_file)\n",
        "#     return model\n",
        "\n",
        "# en_w2v = load(PATH + 'glove.6B.300d.txt')\n",
        "# en_w2v.wv.save_word2vec_format(PATH + 'glove.6B.300d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f82wvwPtJ9X"
      },
      "source": [
        "# 저장한 w2v 모델 불러오고 torchtext.vocab 에 적용하기\n",
        "en_vectors = Vectors(name = 'glove.6B.300d.txt', cache = PATH) # model_name + path = path_to_embeddings_file\n",
        "ENGLISH.build_vocab(train_data, max_size=10000, min_freq=2,\n",
        "                    vectors = en_vectors) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
        "CNN_ENGLISH.build_vocab(train_data, max_size=10000, min_freq=2,\n",
        "                    vectors = en_vectors) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
        "\n",
        "#!# 실제 embedding 결과 확인\n",
        "\n",
        "# embedding layer 에 적용하기\n",
        "en_embedding_layer = nn.Embedding.from_pretrained(ENGLISH.vocab.vectors, freeze = False)\n",
        "en_embedding_layer = nn.Embedding.from_pretrained(CNN_ENGLISH.vocab.vectors, freeze = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2GN1OFutJ9X"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKlcdW_UtJ9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8044bf54-1924-4b77-e2cd-2b59134457ce"
      },
      "source": [
        "## model building\n",
        "# preparation\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpbJpbpatJ9Y"
      },
      "source": [
        "### \\<pack padded\\> seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YknPdiBHtJ9Y"
      },
      "source": [
        "## model 1 : (Packed) Encoder-Decoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "                \n",
        "        #need to explicitly put lengths on cpu!\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "                                 \n",
        "        #packed_outputs is a packed sequence containing all hidden states\n",
        "        #hidden is now from the final non-padded element in the batch\n",
        "            \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "        #outputs is now a non-packed sequence, all hidden states obtained\n",
        "        #  when the input is a pad token are all zeros\n",
        "            \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "  \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention = [batch size, src len]\n",
        "        \n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #mask = [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qMEal2C9dsh"
      },
      "source": [
        "### <2. CNN> seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIfJ_ZGEtJ9Z"
      },
      "source": [
        "## model 2 : Convolutional Seq to Seq\n",
        "class Conv_Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size, \n",
        "                                              padding = (kernel_size - 1) // 2)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, src len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, src len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, src len]\n",
        "        \n",
        "        #begin convolutional blocks...\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(self.dropout(conv_input))\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, src len]\n",
        "\n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "        \n",
        "        #...end convolutional blocks\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved = [batch size, src len, emb dim]\n",
        "        \n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (conved + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        return conved, combined\n",
        "\n",
        "class Conv_Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #embedded = [batch size, trg len, emb dim]\n",
        "        #conved = [batch size, hid dim, trg len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved_emb = [batch size, trg len, emb dim]\n",
        "        \n",
        "        combined = (conved_emb + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, trg len, emb dim]\n",
        "                \n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "        \n",
        "        #energy = [batch size, trg len, src len]\n",
        "        \n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        #attention = [batch size, trg len, src len]\n",
        "            \n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg len, emd dim]\n",
        "        \n",
        "        #convert from emb dim -> hid dim\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        \n",
        "        #attended_encoding = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #apply residual connection\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        \n",
        "        #attended_combined = [batch size, hid dim, trg len]\n",
        "        \n",
        "        return attention, attended_combined\n",
        "        \n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "            \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, trg len]\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #tok_embedded = [batch size, trg len, emb dim]\n",
        "        #pos_embedded = [batch size, trg len, emb dim]\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #embedded = [batch size, trg len, emb dim]\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #conv_input = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #conv_input = [batch size, hid dim, trg len]\n",
        "        \n",
        "        batch_size = conv_input.shape[0]\n",
        "        hid_dim = conv_input.shape[1]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #apply dropout\n",
        "            conv_input = self.dropout(conv_input)\n",
        "        \n",
        "            #need to pad so decoder can't \"cheat\"\n",
        "            padding = torch.zeros(batch_size, \n",
        "                                  hid_dim, \n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
        "                \n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
        "        \n",
        "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(padded_conv_input)\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, trg len]\n",
        "            \n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #calculate attention\n",
        "            attention, conved = self.calculate_attention(embedded, \n",
        "                                                         conved, \n",
        "                                                         encoder_conved, \n",
        "                                                         encoder_combined)\n",
        "            \n",
        "            #attention = [batch size, trg len, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            \n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "            \n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "         \n",
        "        #conved = [batch size, trg len, emb dim]\n",
        "            \n",
        "        output = self.fc_out(self.dropout(conved))\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention\n",
        "\n",
        "class Conv_Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
        "           \n",
        "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
        "        #encoder_conved is output from final encoder conv. block\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
        "        #  positional embeddings \n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "            \n",
        "        #encoder_conved = [batch size, src len, emb dim]\n",
        "        #encoder_combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        #calculate predictions of next words\n",
        "        #output is a batch of predictions for each word in the trg sentence\n",
        "        #attention a batch of attention scores across the src sentence for \n",
        "        #  each word in the trg sentence\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #attention = [batch size, trg len - 1, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77qCgqUO9ueD"
      },
      "source": [
        "### 3. Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAUIU8GZtJ9a"
      },
      "source": [
        "## model 3 : Transformers\n",
        "class Transformer_Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x\n",
        "class Transformer_Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention\n",
        "    \n",
        "class Transformer_Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baY-g2MJtJ9a"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiEqIZM2tJ9b"
      },
      "source": [
        "## training\n",
        "# model 1 : pack padded seq2seq\n",
        "INPUT_DIM   = len(KOREAN.vocab)  #!# same for-pretrained vocab?\n",
        "OUTPUT_DIM  = len(ENGLISH.vocab) #!# same for-pretrained vocab?\n",
        "ENC_EMB_DIM = 256 #!# 300\n",
        "DEC_EMB_DIM = 256 #!# 300\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "KO_PAD_IDX = KOREAN.vocab.stoi[KOREAN.pad_token]\n",
        "EN_PAD_IDX = ENGLISH.vocab.stoi[ENGLISH.pad_token]\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc  = Encoder(INPUT_DIM,  ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec  = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "Seq2Seq_model = Seq2Seq(enc, dec, KO_PAD_IDX, device).to(device)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "Seq2Seq_model.apply(init_weights)\n",
        "\n",
        "optimizer = optim.Adam(Seq2Seq_model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = EN_PAD_IDX)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        ko, ko_len = batch.ko\n",
        "        en = batch.en\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(ko, ko_len, en)\n",
        "        \n",
        "        #en = [en len, batch size]\n",
        "        #output = [en len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        en = en[1:].view(-1)\n",
        "        \n",
        "        #en = [(en len - 1) * batch size]\n",
        "        #output = [(en len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, en)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            ko, ko_len = batch.ko\n",
        "            en = batch.en\n",
        "\n",
        "            output = model(ko, ko_len, en, 0) #turn off teacher forcing\n",
        "            \n",
        "            #en = [en len, batch size]\n",
        "            #output = [en len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            en = en[1:].view(-1)\n",
        "\n",
        "            #en = [(en len - 1) * batch size]\n",
        "            #output = [(en len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, en)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_hErUEntJ9b",
        "outputId": "58ec9ae3-f440-417f-dfa0-0ae09678f993"
      },
      "source": [
        "# model 1 : pack padded seq2seq\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(Seq2Seq_model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = evaluate(Seq2Seq_model, test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(Seq2Seq_model.state_dict(), 'tut4-Seq2Seq_model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {np.exp(test_loss):7.3f}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 2m 4s\n",
            "\tTrain Loss: 6.149 | Train PPL: 468.306\n",
            "\t Test. Loss: 5.825 |  Test. PPL: 338.698\n",
            "Epoch: 02 | Time: 2m 3s\n",
            "\tTrain Loss: 5.223 | Train PPL: 185.469\n",
            "\t Test. Loss: 4.437 |  Test. PPL:  84.519\n",
            "Epoch: 03 | Time: 2m 3s\n",
            "\tTrain Loss: 4.330 | Train PPL:  75.958\n",
            "\t Test. Loss: 4.178 |  Test. PPL:  65.229\n",
            "Epoch: 04 | Time: 2m 2s\n",
            "\tTrain Loss: 4.165 | Train PPL:  64.376\n",
            "\t Test. Loss: 4.133 |  Test. PPL:  62.364\n",
            "Epoch: 05 | Time: 2m 2s\n",
            "\tTrain Loss: 4.082 | Train PPL:  59.248\n",
            "\t Test. Loss: 4.005 |  Test. PPL:  54.855\n",
            "Epoch: 06 | Time: 2m 1s\n",
            "\tTrain Loss: 4.044 | Train PPL:  57.070\n",
            "\t Test. Loss: 4.028 |  Test. PPL:  56.162\n",
            "Epoch: 07 | Time: 2m 1s\n",
            "\tTrain Loss: 4.043 | Train PPL:  56.980\n",
            "\t Test. Loss: 4.001 |  Test. PPL:  54.672\n",
            "Epoch: 08 | Time: 2m 3s\n",
            "\tTrain Loss: 3.949 | Train PPL:  51.901\n",
            "\t Test. Loss: 3.961 |  Test. PPL:  52.508\n",
            "Epoch: 09 | Time: 2m 1s\n",
            "\tTrain Loss: 3.869 | Train PPL:  47.907\n",
            "\t Test. Loss: 3.902 |  Test. PPL:  49.501\n",
            "Epoch: 10 | Time: 2m 1s\n",
            "\tTrain Loss: 3.823 | Train PPL:  45.726\n",
            "\t Test. Loss: 3.943 |  Test. PPL:  51.553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GA2IHS6tJ9b"
      },
      "source": [
        "# model 2 : Conv. seq2seq\n",
        "#!# batch-first 에 주의할 것\n",
        "INPUT_DIM  = len(CNN_KOREAN.vocab)\n",
        "OUTPUT_DIM = len(CNN_ENGLISH.vocab)\n",
        "EMB_DIM = 256 #!# 300\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "EN_PAD_IDX = CNN_ENGLISH.vocab.stoi[CNN_ENGLISH.pad_token]\n",
        "    \n",
        "enc = Conv_Encoder(INPUT_DIM,  EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
        "dec = Conv_Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, EN_PAD_IDX, device)\n",
        "\n",
        "CNN_model = Conv_Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "optimizer = optim.Adam(CNN_model.parameters())\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = EN_PAD_IDX)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        ko = batch.ko\n",
        "        en = batch.en\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(ko, en[:,:-1])\n",
        "        \n",
        "        #output = [batch size, en len - 1, output dim]\n",
        "        #en = [batch size, en len]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        en = en[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * en len - 1, output dim]\n",
        "        #en = [batch size * en len - 1]\n",
        "        \n",
        "        loss = criterion(output, en)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            ko = batch.ko\n",
        "            en = batch.en\n",
        "\n",
        "            output, _ = model(ko, en[:,:-1])\n",
        "        \n",
        "            #output = [batch size, en len - 1, output dim]\n",
        "            #en = [batch size, en len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            en = en[:,1:].contiguous().view(-1)\n",
        "\n",
        "            #output = [batch size * en len - 1, output dim]\n",
        "            #en = [batch size * en len - 1]\n",
        "            \n",
        "            loss = criterion(output, en)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbRddvdptJ9b",
        "outputId": "470d0107-fb94-4ded-d350-04d84c89f386"
      },
      "source": [
        "# model 2 : Conv. seq2seq\n",
        "N_EPOCHS = 10\n",
        "CLIP = 0.1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(CNN_model, CNN_train_iterator, optimizer, criterion, CLIP)\n",
        "#     valid_loss = evaluate(CNN_model, valid_iterator, criterion)\n",
        "    valid_loss = evaluate(CNN_model, CNN_test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save(CNN_model.state_dict(), 'tut5-CNN_model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 5m 32s\n",
            "\tTrain Loss: 4.453 | Train PPL:  85.916\n",
            "\t Val. Loss: 3.746 |  Val. PPL:  42.337\n",
            "Epoch: 02 | Time: 7m 1s\n",
            "\tTrain Loss: 3.786 | Train PPL:  44.072\n",
            "\t Val. Loss: 3.476 |  Val. PPL:  32.333\n",
            "Epoch: 03 | Time: 6m 49s\n",
            "\tTrain Loss: 3.554 | Train PPL:  34.943\n",
            "\t Val. Loss: 3.313 |  Val. PPL:  27.462\n",
            "Epoch: 04 | Time: 6m 36s\n",
            "\tTrain Loss: 3.374 | Train PPL:  29.201\n",
            "\t Val. Loss: 3.065 |  Val. PPL:  21.440\n",
            "Epoch: 05 | Time: 7m 5s\n",
            "\tTrain Loss: 3.236 | Train PPL:  25.428\n",
            "\t Val. Loss: 3.014 |  Val. PPL:  20.378\n",
            "Epoch: 06 | Time: 7m 16s\n",
            "\tTrain Loss: 3.192 | Train PPL:  24.346\n",
            "\t Val. Loss: 2.884 |  Val. PPL:  17.889\n",
            "Epoch: 07 | Time: 7m 36s\n",
            "\tTrain Loss: 3.367 | Train PPL:  28.987\n",
            "\t Val. Loss: 3.529 |  Val. PPL:  34.093\n",
            "Epoch: 08 | Time: 8m 4s\n",
            "\tTrain Loss: 8.847 | Train PPL: 6950.111\n",
            "\t Val. Loss: 5.695 |  Val. PPL: 297.392\n",
            "Epoch: 09 | Time: 7m 29s\n",
            "\tTrain Loss: 2.972 | Train PPL:  19.528\n",
            "\t Val. Loss: 2.616 |  Val. PPL:  13.686\n",
            "Epoch: 10 | Time: 7m 45s\n",
            "\tTrain Loss: 2.947 | Train PPL:  19.051\n",
            "\t Val. Loss: 2.562 |  Val. PPL:  12.955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktrf1gzitJ9c"
      },
      "source": [
        "# model 3 : transformer\n",
        "INPUT_DIM = len(CNN_KOREAN.vocab)\n",
        "OUTPUT_DIM = len(CNN_ENGLISH.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Transformer_Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Transformer_Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "KO_PAD_IDX = KOREAN.vocab.stoi[KOREAN.pad_token]\n",
        "EN_PAD_IDX = ENGLISH.vocab.stoi[ENGLISH.pad_token]\n",
        "\n",
        "transformer_model = Transformer_Seq2Seq(enc, dec, KO_PAD_IDX, EN_PAD_IDX, device).to(device)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "transformer_model.apply(initialize_weights)\n",
        "\n",
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = EN_PAD_IDX)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        ko = batch.ko\n",
        "        en = batch.en\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(ko, en[:,:-1])\n",
        "                \n",
        "        #output = [batch size, en len - 1, output dim]\n",
        "        #en = [batch size, en len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        en = en[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        #output = [batch size * en len - 1, output dim]\n",
        "        #en = [batch size * en len - 1]\n",
        "            \n",
        "        loss = criterion(output, en)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            ko = batch.ko\n",
        "            en = batch.en\n",
        "\n",
        "            output, _ = model(ko, en[:,:-1])\n",
        "            \n",
        "            #output = [batch size, en len - 1, output dim]\n",
        "            #en = [batch size, en len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            en = en[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * en len - 1, output dim]\n",
        "            #en = [batch size * en len - 1]\n",
        "            \n",
        "            loss = criterion(output, en)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShuxrfEstJ9c",
        "outputId": "a1b68829-e7da-48d6-aad5-f748018d106d"
      },
      "source": [
        "# model 3 : transformer\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(transformer_model, CNN_train_iterator, optimizer, criterion, CLIP)\n",
        "#     valid_loss = evaluate(transformer_model, valid_iterator, criterion)\n",
        "    valid_loss = evaluate(transformer_model, CNN_test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "#    if valid_loss < best_valid_loss:\n",
        "#        best_valid_loss = valid_loss\n",
        "#        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {np.exp(valid_loss):7.3f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 29s\n",
            "\tTrain Loss: 4.483 | Train PPL:  88.472\n",
            "\t Val. Loss: 3.625 |  Val. PPL:  37.534\n",
            "Epoch: 02 | Time: 0m 28s\n",
            "\tTrain Loss: 3.429 | Train PPL:  30.854\n",
            "\t Val. Loss: 2.952 |  Val. PPL:  19.154\n",
            "Epoch: 03 | Time: 0m 29s\n",
            "\tTrain Loss: 2.862 | Train PPL:  17.494\n",
            "\t Val. Loss: 2.342 |  Val. PPL:  10.407\n",
            "Epoch: 04 | Time: 0m 29s\n",
            "\tTrain Loss: 2.372 | Train PPL:  10.721\n",
            "\t Val. Loss: 1.999 |  Val. PPL:   7.378\n",
            "Epoch: 05 | Time: 0m 29s\n",
            "\tTrain Loss: 2.074 | Train PPL:   7.958\n",
            "\t Val. Loss: 1.684 |  Val. PPL:   5.386\n",
            "Epoch: 06 | Time: 0m 29s\n",
            "\tTrain Loss: 1.827 | Train PPL:   6.213\n",
            "\t Val. Loss: 1.437 |  Val. PPL:   4.209\n",
            "Epoch: 07 | Time: 0m 29s\n",
            "\tTrain Loss: 1.603 | Train PPL:   4.969\n",
            "\t Val. Loss: 1.219 |  Val. PPL:   3.385\n",
            "Epoch: 08 | Time: 0m 29s\n",
            "\tTrain Loss: 1.414 | Train PPL:   4.113\n",
            "\t Val. Loss: 1.058 |  Val. PPL:   2.881\n",
            "Epoch: 09 | Time: 0m 29s\n",
            "\tTrain Loss: 1.246 | Train PPL:   3.478\n",
            "\t Val. Loss: 0.896 |  Val. PPL:   2.449\n",
            "Epoch: 10 | Time: 0m 29s\n",
            "\tTrain Loss: 1.116 | Train PPL:   3.053\n",
            "\t Val. Loss: 0.779 |  Val. PPL:   2.180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO6Dsnb0tJ9c"
      },
      "source": [
        "## result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhxgHY2atJ9c"
      },
      "source": [
        "## Bleu Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a_yVZqYtJ9d"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)])\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n",
        "\n",
        "def CNN_translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_conved, encoder_combined = model.encoder(src_tensor)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention\n",
        "\n",
        "def transformer_translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('de_core_news_sm')\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention\n",
        "\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, translate_sentence, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['ko']\n",
        "        trg = vars(datum)['en']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYbFfSBVtJ9d",
        "outputId": "dca811d6-8e1e-4e1a-da6a-c3afca0ca773"
      },
      "source": [
        "## pack padded\n",
        "seq_bleu_score = calculate_bleu(test_data, KOREAN, ENGLISH, Seq2Seq_model, device, translate_sentence)\n",
        "print(f'BLEU score = {seq_bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 9.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw4uMuBJtJ9d",
        "outputId": "9be6c0a1-75aa-4b9c-f3aa-7ff838036a16"
      },
      "source": [
        "## CNN\n",
        "cnn_bleu_score = calculate_bleu(test_data, CNN_KOREAN, CNN_ENGLISH, CNN_model, device, CNN_translate_sentence, max_len = 50)\n",
        "print(f'BLEU score = {cnn_bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 6.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HK2FRmT6tJ9e",
        "outputId": "a8ff3d26-594b-4a57-9ea1-c7e1d64af36c"
      },
      "source": [
        "## transformer\n",
        "transformer_bleu_score = calculate_bleu(test_data, CNN_KOREAN, CNN_ENGLISH, transformer_model, device, transformer_translate_sentence, max_len = 50)\n",
        "print(f'BLEU score = {transformer_bleu_score*100:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score = 32.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcBRAalStJ9e"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C10wYZH4tJ9e"
      },
      "source": [
        "## 형태소 분석을 할 경우\n",
        "\n",
        "sen_list = [\n",
        "'모든 액체 , 젤 , 에어로졸 등 은 1 커트 짜리 여닫이 투명 봉지 하나 에 넣 어야 하 ㅂ니다 .',\n",
        "'미안 하 지만 , 뒷쪽 아이 들 의 떠들 는 소리 가 커 어서 , 광화문 으로 가 아고 싶 은데 표 를 바꾸 어 주 시 겠 어요 ?',\n",
        "'은행 이 너무 멀 어서 안 되 겠 네요 . 현찰 이 필요 하면 돈 을 훔치 시 어요',\n",
        "'아무래도 분실 하 ㄴ 것 같 으니 분실 신고서 를 작성 하 아야 하 겠 습니다 . 사무실 로 같이 가 시 ㄹ 까요 ?',\n",
        "'부산 에서 코로나 확진자 가 급증 하 아서 병상 이 부족하 아 지자  확진자 20명 을 대구 로 이송하 ㄴ다 .',\n",
        "'변기 가 막히 었 습니다 .',\n",
        "'그 바지 좀 보이 어 주 시 ㅂ시오 . 이거 얼마 에 사 ㄹ 수 있 는 것 이 ㅂ니까 ?',\n",
        "'비 가 오 아서 백화점 으로 가지 말 고 두타 로 가 았 으면 좋 겠 습니다 .',\n",
        "'속 이 안 좋 을 때 는 죽 이나 미음 으로 아침 을 대신 하 ㅂ니다',\n",
        "'문 대통령 은 집단 이익 에서 벗어 나 아 라고 말 하 었 다 .',\n",
        "'이것 좀 먹어 보 ㄹ 몇 일 간 의 시간 을 주 시 어요 .',\n",
        "'이날 개미군단 은 외인 의 물량 을 모두 받 아 내 었 다 .',\n",
        "'통합 우승 의 목표 를 달성하 ㄴ NC 다이노스 나성범 이 메이저리그 진출 이라는 또 다른 꿈 을 향하 어 나아가 ㄴ다 .',\n",
        "'이번 구조 조정 이 제품 을 효과 적 으로 개발 하 고 판매 하 기 위하 ㄴ 회사 의 능력 강화 조처 이 ㅁ 을 이해 하 아 주 시 리라 생각 하 ㅂ니다 .',\n",
        "'요즘 이 프로그램 녹화 하 며 많은 걸 느끼 ㄴ다 '\n",
        "'배낭 여행 은 우리 들 이 어리었을 때 허용 되 지 않 으면 우울 하 아 하 ㄴ다'\n",
        "'그 소녀 는 단지 늑대 처럼 울부짖 을 수 있 을 뿐 이 었 다']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ov9J4-YtJ9e"
      },
      "source": [
        "## 어절단위로 결합하여 할 경우\n",
        "\n",
        "sen_list = [\n",
        "'모든 액체, 젤, 에어로졸 등은 1커트 짜리 여닫이 투명 봉지 하나에 넣어야 합니다.',\n",
        "'미안하지만 , 뒷쪽 아이들의 떠드는 소리가 커서 , 광화문으로 가고 싶은데 표를 바꾸어 주시겠어요?',\n",
        "'은행이 너무 멀어서 안되겠네요. 현찰이 필요하면 돈을 훔치세요',\n",
        "'아무래도 분실한 것 같으니 분실신고서를 작성해야 하겠습니다. 사무실로 같이 가실까요?',\n",
        "'부산에서 코로나 확진자가 급증해서 병상이 부족해지자  확진자 20명을 대구로 이송한다.',\n",
        "'변기가 막히었습니다 .',\n",
        "'그 바지 좀 보여주십시오. 이거 얼마에 살 수 있는 것입니까?',\n",
        "'비가 와서 백화점으로 가지 말고 두타로 갔으면 좋겠습니다 .',\n",
        "'속이 안좋을 때는 죽이나 미음으로 아침을 대신 합니다',\n",
        "'문 대통령은 집단이익에서 벗어나라고 말하였다.',\n",
        "'이것 좀 먹어 볼 몇 일 간의 시간을 주세요.',\n",
        "'이날 개미군단은 외인의 물량을 모두 받아내었다.',\n",
        "'통합 우승의 목표를 달성한 NC 다이노스 나성범이 메이저리그 진출이라는 또다른 꿈을 향해 나아간다.',\n",
        "'이번 구조 조정이 제품을 효과적으로 개발하고 판매하기 위한 회사의 능력 강화 조처임을 이해해 주시리라 생각합니다.',\n",
        "'요즘 이 프로그램 녹화하며 많은 걸 느낀다'\n",
        "'배낭 여행은 우리들이 어렸을 때 허용디지 않으면 우울 해 한다'\n",
        "'그 소녀는 단지 늑대처럼 울부짖을 수 있을 뿐이었다']\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}