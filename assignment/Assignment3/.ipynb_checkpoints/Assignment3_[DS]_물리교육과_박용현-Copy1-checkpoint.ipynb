{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목표\n",
    "\n",
    "- csv 파일을 읽어서 torchtext를 사용하여 데이터를 신경망에 입력가능한 꼴로 바꾸기\n",
    "(Field, Iterator, train,test, evaluation and prediction)\n",
    "- base line으로 Naive Bayes classification 구현\n",
    "- 한국어 데이터 전처리를 위한 함수를 만들고 이를 torchtext에 통합하기 \n",
    "- 제시된 여러 모델을 사용하여(transformers 제외) 성능을 향상 시키기\n",
    "- training, evaluation 한 것을 test 데이터에 적용하여 성능을 보이기.\n",
    "- predict를 사용하여 제시된 기사들의 분류 결과를 보이기\n",
    "\n",
    "- 참고 사이트\n",
    "    - https://pytorch.org/text/\n",
    "    - http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "    - https://github.com/pytorch/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내용\n",
    "\n",
    "- 첨부된 BalancedNewsCorpus_train.csv, BalancedNewsCorpus_test.csv는 국어원 뉴스 자료에서 9개 분야의 신문별 균형을 맞춘 자료로, 학습용 9,000개 시험용 1800 자료가 있는 파일이다.\n",
    "- 이 파일을 가지고 https://github.com/bentrevett/pytorch-sentiment-analysis 에 있는 pytorch sentiment analysis의 방법을 따라 한국어 뉴스기사 분류기를 만들어라\n",
    "- 한국어 선처리를 위해 함수를 만들어 이를 torchText에 통합하여 사용. preprocessing은 다양한 방법으로 가능함.\n",
    "- baseline으로 Naive Bayes를 사용하고 Neural Network를 사용하는 모델이 얼마나 더 성능의 향상을 이루는지 보여라..\n",
    "- https://github.com/bentrevett/pytorch-sentiment-analysis 에 제시된 방법 중 가장 성능이 좋은 방법을 사용할 수 있음. **단 이 과제에서는 외부 임베딩과, transformers를 사용하는 방법은 적용하지 말것**\n",
    "- Evaluation, Test 성능을 정리하고, 이렇게 학습한 모델로 제시된 User Input에서 제시된 문장의 출력과 정답을 비교 분석하라.\n",
    "- 화일 이름은 MidTermProject_DS(or CL)_Group X\n",
    "- 조원 이름 명시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "- 마감: 10월 21일 목요일 오후 12시!\n",
    "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 Assignment3_[DS또는 CL]_학과_이름.ipynb\n",
    "- 화일에 각 조원 이름 명시\n",
    "- 코드, 또는 셀 마다 자세한 설명 요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "- 구현한 시스템의 성능을 정리 (프리프로세싱 방법, 사용한 모델, 테스트 성능 등)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code starts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import einops # for using rearrange function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import dill\n",
    "\n",
    "import time\n",
    "\n",
    "import re\n",
    "from eunjeon import Mecab\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv 파일을 읽어서 torchtext를 사용하여 데이터를 신경망에 입력가능한 꼴로 바꾸기 (Field, Iterator, train, test, evaluation and prediction)\n",
    "\n",
    "base line으로 Naive Bayes classification 구현\n",
    "\n",
    "한국어 데이터 전처리를 위한 함수를 만들고 이를 torchtext에 통합하기\n",
    "\n",
    "제시된 여러 모델을 사용하여(transformers 제외) 성능을 향상 시키기\n",
    "\n",
    "training, evaluation 한 것을 test 데이터에 적용하여 성능을 보이기.\n",
    "\n",
    "predict를 사용하여 제시된 기사들의 분류 결과를 보이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- stopword_list ----- \n",
      " ['‥', '․', '�', '㎃', '“', 'ⓝ', '◇', ',', '㎏', '★', '㎢', '℃', '㎥', '∙', '㏅', '#', '－', '⊙', '~', '＂', '\\x9f', \"'\", '㎞', '@', '＋', '′', '±', '\"', '♪', '㎍', '…', '▶', '㏊', '㎝', '＆', '＇', '【', '<', '］', '○', '\\xa0', '〕', '●', '&', '△', '㎹', '．', ']', ';', '』'] ...etc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-6c870c7e2c7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'News'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'news'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNEWS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Label'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m train_data, test_data = data.TabularDataset.splits(\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mpath\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;34m'./'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'BalancedNewsCorpus_train_labeled.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchtext\\legacy\\data\\dataset.py\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         train_data = None if train is None else cls(\n\u001b[0m\u001b[0;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n\u001b[0;32m     79\u001b[0m         val_data = None if validation is None else cls(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchtext\\legacy\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchtext\\legacy\\data\\dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchtext\\utils.py\u001b[0m in \u001b[0;36municode_csv_reader\u001b[1;34m(unicode_csv_data, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfield_size_limit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxInt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode_csv_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## preprocessing\n",
    "train = pd.read_csv('BalancedNewsCorpus_train.csv')\n",
    "test  = pd.read_csv('BalancedNewsCorpus_test.csv')\n",
    "\n",
    "# 먼저 label column 을 만들고, Topic 에 대응하는 index number를 저장해줬다. \n",
    "Topic_dict = {'IT/과학': 0, '경제': 1, '문화': 2, '미용/건강': 3, '사회': 4, '생활': 5, '스포츠': 6, '연예': 7, '정치': 8}\n",
    "\n",
    "train['Label'] = [Topic_dict[Topic] for Topic in train['Topic']]\n",
    "test['Label']  = [Topic_dict[Topic] for Topic in test['Topic']]\n",
    "\n",
    "train.to_csv('BalancedNewsCorpus_train_labeled.csv', index = False)\n",
    "test.to_csv('BalancedNewsCorpus_test_labeled.csv', index = False)\n",
    "\n",
    "train_labeled = pd.read_csv('BalancedNewsCorpus_train_labeled.csv')\n",
    "test_labeled = pd.read_csv('BalancedNewsCorpus_test_labeled.csv')\n",
    "train_labeled.head()\n",
    "\n",
    "## define torchtext.data.Field\n",
    "# tokenizer로는 은전한닢 프로젝트의 Mecab 을 사용했다.\n",
    "tokenizer = Mecab()\n",
    "\n",
    "# define stopwords\n",
    "news_list = list(train['News']) + list(test['News'])\n",
    "stopword_set = set()\n",
    "for news in news_list:\n",
    "    stopword_set = stopword_set.union(set(re.findall(r'[\\W]', news))) # 특수기호\n",
    "stopword_list = list(stopword_set)\n",
    "\n",
    "with open(\"stopwords.json\", 'r') as f: # frequently used stopwords (https://www.ranks.nl/stopwords/korean)\n",
    "    stopwords = json.load(f)\n",
    "    stopword_list += stopwords\n",
    "    \n",
    "stopword_list += ['p', '</', 'ㆍ','0','1','2','3','4','5','6','7','8','9'] # BOS/EOS and single digit number\n",
    "\n",
    "print('\\n----- stopword_list ----- \\n', stopword_list[:50], '...etc')\n",
    "\n",
    "NEWS = data.Field(\n",
    "    sequential = True,\n",
    "    use_vocab = True,\n",
    "    is_target = False,\n",
    "    tokenize = tokenizer.morphs,\n",
    "    batch_first = False, #!#\n",
    "    stop_words = stopword_list,\n",
    "    include_lengths = True,\n",
    "    fix_length = 500 # maximum length = 492\n",
    ")\n",
    "\n",
    "LABEL = data.Field(\n",
    "    sequential = False,\n",
    "    use_vocab = False,\n",
    "    is_target = True,\n",
    "    dtype = torch.float\n",
    ")\n",
    "\n",
    "fields = {'News' : ('news', NEWS), 'Label' : ('label', LABEL)}\n",
    "\n",
    "train_data, test_data = data.TabularDataset.splits(\n",
    "    path  = './',\n",
    "    train = 'BalancedNewsCorpus_train_labeled.csv',\n",
    "    test  = 'BalancedNewsCorpus_test_labeled.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields\n",
    ")\n",
    "\n",
    "NEWS.build_vocab(train_data, max_size = 35000, min_freq = 10)\n",
    "print('\\n----- vocab_list ----- \\n', NEWS.vocab.itos[0:10])\n",
    "\n",
    "# iterator for NN model \n",
    "batch_size = 64\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = batch_size,\n",
    "    sort_key = lambda x: len(x.news), # https://stackoverflow.com/questions/58241313/understanding-typeerror-not-supported-between-instances-of-example-and-e\n",
    "    sort_within_batch=True\n",
    ")\n",
    "\n",
    "# iterator for Naive Bayes model\n",
    "NB_batch_size = len(train_data)\n",
    "NB_train_iterator, NB_test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = NB_batch_size,\n",
    "    sort = False # https://github.com/pytorch/text/issues/474\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NEWS.Field\",\"wb\")as f:\n",
    "     dill.dump(NEWS,f)\n",
    "\n",
    "with open(\"model/TEXT.Field\",\"wb\")as f:\n",
    "     dill.dump(TEXT,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.284\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "for batch in NB_train_iterator:\n",
    "    news  = einops.rearrange(batch.news[0], 'i j -> j i')\n",
    "    label = batch.label\n",
    "    clf.fit(news, label)\n",
    "    \n",
    "for batch in NB_test_iterator:\n",
    "    news  = einops.rearrange(batch.news[0], 'i j -> j i')\n",
    "    label = batch.label\n",
    "    print('Accuracy: %.3f' % clf.score(news, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 559,  552,  924,  ..., 8028, 1203,  484])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, news):\n",
    "        embedded = self.embedding(news)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = True, \n",
    "                           dropout = dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        # lengths need to be on CPU!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "    \n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.news\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.news\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your choice of Neural Network Model\n",
    "INPUT_DIM = len(NEWS.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = NEWS.vocab.stoi[NEWS.pad_token]\n",
    "\n",
    "model = LSTM(INPUT_DIM, \n",
    "             EMBEDDING_DIM, \n",
    "             HIDDEN_DIM, \n",
    "             OUTPUT_DIM, \n",
    "             N_LAYERS, \n",
    "             DROPOUT, \n",
    "             PAD_IDX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "####  뉴스 labels\n",
    "    -  IT/과학': 0, '경제': 1, '문화': 2, '미용/건강': 3, '사회': 4, '생활': 5, '스포츠': 6, '연예': 7, '정치': 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_news(model, sentence, min_len=5):\n",
    "    global NEWS, tokenizer, device, PAD_IDX\n",
    "    model.eval()\n",
    "    \n",
    "    tokenized = [tok for tok in tokenizer.morphs(sentence)]\n",
    "    indexed = [NEWS.vocab.stoi[t] for t in tokenized]\n",
    "    length  = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    \n",
    "    return prediction.item()\n",
    "    \n",
    "def predict_news_NB(model, sentence, min_len=5):\n",
    "    global NEWS, tokenizer\n",
    "    \n",
    "    PAD_IDX = NEWS.vocab.stoi[NEWS.pad_token]\n",
    "    \n",
    "    tokenized = [tok for tok in tokenizer.morphs(sentence)]\n",
    "    indexed = [NEWS.vocab.stoi[t] for t in tokenized]\n",
    "    \n",
    "    while len(indexed) < 500:\n",
    "        indexed.append(PAD_IDX)\n",
    "    \n",
    "    length  = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    tensor = einops.rearrange(tensor, 'i j -> j i')\n",
    "    \n",
    "    return model.predict(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 아래 문장의 정답은 8-4-1-3-5-6-0-2-7\n",
    "## 연예/문화, 정치/경제/사회/생활 등 명확히 구별되기 어려운 범주들이 있음...\n",
    "\n",
    "sentence1 = \"여러 차례 선거를 치르며 조직적인 지지모임과 온라인 팬덤을 보유한 이 지사에 비해 부족하다는 것이다. 윤석열 캠프에선 오차범위를 다투는 여론조사 지지율과는 별개로, ‘조직’에 있어선 아직도 채워야 할 부분이 많다는 이야기가 나온다. 윤석열 캠프 관계자는 “사람만 많이 모은다고 좋을 게 없다는 지적을 듣기도 하지만, 인구 1300만명의 지자체장인 이재명 지사에 비하면 많은 게 아닌 상황”이라고 말했다.\"\n",
    "sentence2 = \"여자친구가 이별을 통보하고 새 남자친구를 사귀자 지속적으로 찾아가 협박과 폭행을 가한 30대 남성이 실형을 선고 받았다. 창원지법 형사4단독 안좌진 부장판사는 상해, 주거침입, 폭행 등 혐의로 재판에 넘겨진 A(39)씨에게 징역 1년3개월을 선고했다고 10일 밝혔다.A씨는 지난해 4월부터 올해 2월까지 10개월 가량 사귄 B씨(30)가 이별을 통보하자 지난 3월 6일 B씨 집을 찾아가 욕설을 퍼붓고 B씨를 폭행한 혐의를 받고 있다.\"\n",
    "sentence3 = \"동탄신도시의 성공은 명실상부한 한국 1위의 기업 삼성전자를 빼놓고는 설명할 수 없다. 삼성전자는 수출의 20%를 담당하는 한국 경제의 심장이다. 삼성전자의 연구소와 공장은 세계 최고 수준의 연구인력과 협력업체를 끌어당기는 블랙홀이다.동탄신도시 인근에 삼성전자 기흥캠퍼스가 있고 화성캠퍼스가 신도시에 자리 잡고 있다. 삼성 화성캠퍼스에서는 메모리와 파운드리 반도체의 설계 및 생산이 이뤄지고 있다.\"\n",
    "sentence4 = \"샤워나 목욕 중에는 물, 샤워타올, 수건 등 균이 닿을 여지가 많다. 샤워를 하는 화장실에는 보통 변기도 함께 있어 배변 활동으로 나온 균이 공기 중을 돌아다니고 있다. 습기가 높아 곰팡이가 생기기도 좋은 환경이다. 화장실에 걸린 샤워타올과 수건이 제대로 건조되지 않은 채 화장실에 내내 있었다면 균이 있을 가능성이 크다. 이 균이 예방 접종 하면서 생긴 손상 부위에 닿으면 드물지만 침입해 감염증을 유발할 수 있다.\"\n",
    "sentence5 = \"식전주의 시간이다. 밥을 먹기 전에 마시는 술. 안주와 함께 먹지 않는 술. 술만으로 온전한 술. 이게 식전주다. 3시와 5시 사이는 식전주의 시간이기도 한 것이다. 이 시간에 마시는 식전주를 나는 꽤나 좋아한다. 술은 다 각각의 매력이 있고, 슬플 때도 기쁠 때도 지루할 때도 피곤할 때도 좋지만, 식전주의 시간에 마시는 식전주도 좋다. 주로 맥주이지만 가끔은 아페리티프(Aperitif·식전주)를 마신다.\"\n",
    "sentence6 = \"시리아전을 마친 뒤 9일 이란으로 출국한 한국 대표팀은 한국 시간 기준 10일 오전 1시경 테헤란 공항에 도착해 숙소인 파르시안 아자디 호텔로 이동했다. 이후 코로나19 PCR 검사를 진행했고, 결과가 나올 때까지 각자 방에서 격리한 채 대기할 예정이다. 한국은 역대 이란 원정에서 한차례도 승리하지 못한 채 2무 5패를 기록 중이다.  선수들이 좋은 컨디션을 유지할 수 있도록 전세기를 마련해 이란으로 향했다.\"\n",
    "sentence7 = \"애플의 아이폰13 시리즈가 지난 8일 국내 판매를 시작했다. 애플이 지난달 14일(현지시각) 신제품을 공개한 후 3주 만이다. 애플은 아이폰13의 두뇌에 해당하는 프로세서와 카메라 성능을 크게 개선했다고 밝혔다. 팀 쿡 애플 최고경영자(CEO)는 “역사상 최고의 아이폰이다”라고 했다. 하지만 전작인 아이폰12와 비교해 큰 차이를 느낄 수 없다는 부정적인 평가도 많다.\"\n",
    "sentence8 = \"극단 마실은 문화체육관광부와 지역문화진흥원 지원으로 '심청전-할머니의 비밀레시피' 온라인 만남 행사를 진행했다고 10일 밝혔다. 행사는 할머니만의 레시피로 함께 음식을 만들며 할머니의 이야기를 공유하고, 할머니를 주인공으로 한 짤막한 연극을 펼치는 순서로 진행됐다. 극단은 지역 내 관음사 연기 설화가 심청전과 연관 있는 점을 토대로 심청의 일생과 닮은 곡성 할머니들의 이야기를 2018년도부터 수집해 연극을 만들었다.\"\n",
    "sentence9 = \"‘놀면 뭐하니?+’에서는 유재석, 정준하, 하하, 신봉선, 미주의 깜짝 ‘꼬치꼬치 기자간담회’와 MBC 보도국 열혈 신입기자로 변신한 ‘뉴스데스크’ 특집이 시작됐다. ‘꼬치꼬치 기자간담회’에서는 정준하가 ‘스포츠 꼬치꼬치’ 기자로 변신, 시청자의 궁금증을 풀어주는 마성의 돌직구 질문을 던졌고, ‘놀면 뭐하니?+’ 멤버들은 솔직한 마음이 담긴 답변으로 큰 웃음과 훈훈함을 동시에 선사했다.\"\n",
    "\n",
    "sentence_list = [globals()['sentence{}'.format(i)] for i in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_NN_list = []\n",
    "predict_NB_list = []\n",
    "\n",
    "for sentece in sentence_list:\n",
    "    predict_NB_list.append(predict_news_NB(clf, sentece))\n",
    "    # predict_NN_list(predict_news(model, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32),\n",
       " array([7.], dtype=float32)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_NB_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
