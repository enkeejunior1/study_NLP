{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Assignment 2: Cross Entropy\n",
    "\n",
    "## Goal\n",
    "- Language Modeling에 따른 Cross Entropy 구현\n",
    "- N-gram Generation, Word Counting, Cleaning Revisit\n",
    "- Smoothing (Add-1) implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "In Information Theory, entropy (denoted $H(X)$) of a random variable X is the expected log probabiltiy:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(X) = - \\sum P(x)log_2 P(x)\n",
    "\\end{equation}\n",
    "\n",
    "and is a measure of uncertainty. \n",
    "\n",
    "\n",
    "### Defn: Cross Entropy\n",
    "\n",
    "The cross entropy, H(p,m), of a true distribution **p** and a model distribution **m** is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "    H(p,m) = - \\sum_{x} p(x) log_2 m(x)\n",
    "\\end{equation}\n",
    "\n",
    "The lower the cross entropy is the closer it is to the true distribution.\n",
    "\n",
    "## Contents\n",
    "- Assignment1에서 사용했던 NIRW1900000011.json 은 전자신문 뉴스기사이다. (training data로 사용)\n",
    "- NWRW1800000045.json 은 동아일보 뉴스 기사이다. (test data로 사용)\n",
    "- 국립국어원의 웹 코퍼스 (WEB) 중의 하나인 EBRW1908000138.json (첨부)은 블로그 자료이다. (test data로 사용)\n",
    "- training data에서 학습한 한글 자소/글자(음절)/어절 별 unigram, Bigram, trigram 모델이 같은 신문기사와 웹자료에 얼마나 잘 부합하는지를 교차 엔트로피로 살펴봄\n",
    "- 세 데이터에서 \"form\"에 해당하는 부분만을 각각 추출하여, 한글 글자들만 남긴 후 (스페이스도 고려) unigram, bigram, trigram 구성을 만들고 빈도를 구함\n",
    "\n",
    "- training 코퍼스에서는 entropy와 cross entropy는 같고 따라서 그 차이는 0이다\n",
    "- 코퍼스에서 테스트 하기 위한 테스트 코퍼스의 교차엔트로피는 각 모델의 확률을 구하고 이를 교차 엔트로피 공식에 따라 구하면 되는데, **이 경우 P(x)는 이 test 코퍼스의 자소별/글자별/어절별 unigram/bigram/trigram의 확률이고 모델의 확률인 logp(m)은 training 코퍼스인 코퍼스에서 구해진 각 모델의 확률이다.** 각 글자별로 이를 다 곱해서 더 하면 교차엔트로피가 구해진다. 즉 training테스트에서 설정한 언어모델이 test 코퍼스에 더 부합할수록 test에서의 각 구성의 확률이 training의 해당 확률에 근접하게 될 것임. 완벽한 경우 두 모델이 일치한다면, 즉 교차엔트로피가 실제 엔트로피와 동일하게 되면 그 차이는 0이 된다. 따라서 H(P,m) - H(p)의 차이가 작을수록 더 좋은 모델이 된다. \n",
    "- 이 경우 training 코퍼스에 없는 n-gram 구성이  test 코퍼스에 있을 경우 문제가 되니 이 구성의 확률을 얻기 위해 ADD-1을 사용해서 smoothing하라.\n",
    "(힌트: training 데이터의 각 n-gram모델의 구성과 test-data의 n-gram모델의 구성을 비교하여 빠져 있는 구성을 보충하고 add-1을 사용해서 확률을 구함)\n",
    "\n",
    "## General\n",
    "- 마감: 10월 7일 목요일 오후 12시!\n",
    "- 이 노트북 화일에 이름을 변경하여 작업하고 제출. 제출시 화일명을 Assignment2_[DS또는 CL]_학과_이름.ipynb\n",
    "- 화일에 각 조원 이름 명시\n",
    "- 코드, 또는 셀 마다 자세한 설명 요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current OS: win32\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, warnings\n",
    "from sys import platform\n",
    "from tqdm import tqdm\n",
    "from soynlp.hangle import decompose\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "from nltk.util import flatten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(f'Current OS: {platform}')\n",
    "\n",
    "FILE_NAME = 'NIRW1900000011.json' ; test_NAME = 'NWRW1800000045.json' ; test2_NAME = 'EBRW1908000138.json'\n",
    "\n",
    "def load_data(name):\n",
    "    with open(os.path.join(os.getcwd(), name), 'r', encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    cle = []\n",
    "    for documents in tqdm(data['document']):\n",
    "        for document in documents['paragraph']:\n",
    "            cle.append(document['form'])\n",
    "    return cle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_huniv2:\n",
    "    def __init__(self, input=None, sample=False):\n",
    "        self.input = input\n",
    "        self.sample = sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocessing(input_text):\n",
    "        output = re.sub('[^ㄱ-ㅣ가-힣]', '', input_text)\n",
    "        return output\n",
    "    \n",
    "    def preprocessing2(input_text):\n",
    "        output = re.sub('[^ㄱ-ㅣ가-힣\\s]', '', input_text)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def list_cleaning(ls, type=1):\n",
    "        temp = []\n",
    "        for sentence in ls:\n",
    "            if type == 1:\n",
    "                temp2 = RE_huniv2.preprocessing(sentence)\n",
    "            elif type == 2:\n",
    "                temp2 = RE_huniv2.preprocessing2(sentence)\n",
    "            temp.append(temp2)\n",
    "        return temp\n",
    "\n",
    "\n",
    "\n",
    "class ngram_huni:\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def fit(ls, N=1, split_type='글자'):   # sklearn style로 만들 수 없을까?\n",
    "        assert type(ls[0]) == str\n",
    "        basic= Counter()\n",
    "        if split_type=='글자':\n",
    "            if N == 1:\n",
    "                for sentence in ls:\n",
    "                    assert type(sentence) == str \n",
    "                    for letter in sentence: \n",
    "                        assert len(letter) == 1\n",
    "                        basic.update(letter) # 글자를 계산해서 업데이트\n",
    "                            \n",
    "            elif N==2:\n",
    "                for sentence in ls:\n",
    "                    assert type(sentence) == str\n",
    "                    # print(sentence)\n",
    "                    temp = list(zip(sentence, sentence[1:]))\n",
    "                    basic.update(temp)\n",
    "                \n",
    "            elif N==3: \n",
    "                for sentence in ls:\n",
    "                    temp = list(zip(sentence, sentence[1:], sentence[2:]))\n",
    "                    basic.update(temp)\n",
    "        \n",
    "        elif split_type == '자모':\n",
    "            if N==1:\n",
    "                for sentence in ls:\n",
    "                    for letter in sentence:\n",
    "                        decoded = decompose(letter) # 글자별로 자모쪼개기\n",
    "                        for jamo in decoded:\n",
    "                            basic.update(jamo) # 하나의 자모별로 업데이트\n",
    "            elif N==2:\n",
    "                temp = []\n",
    "                for sentence in ls:\n",
    "                    for letter in sentence:\n",
    "                        decoded = decompose(letter) # 초중종 구분하지 않음: '김밥' -> (ㄱ, ㅣ), (ㅣ, ㅁ), (ㅁ, ㅂ) \n",
    "                        temp.append(decoded)\n",
    "                flattened = flatten(temp)\n",
    "                window = list(zip(flattened, flattened[1:]))\n",
    "                basic.update(window) \n",
    "            \n",
    "            elif N==3:\n",
    "                temp = []\n",
    "                for sentence in ls:\n",
    "                    for letter in sentence:\n",
    "                        decoded = decompose(letter) # 초중종 구분하지 않음: '김밥' -> (ㄱ, ㅣ), (ㅣ, ㅁ), (ㅁ, ㅂ)\n",
    "                        temp.append(decoded)\n",
    "                flattened = flatten(temp)\n",
    "                window = list(zip(flattened, flattened[1:], flattened[2:]))\n",
    "                basic.update(window) \n",
    "            \n",
    "        elif split_type=='어절':\n",
    "            temp = []\n",
    "            for sentence in ls:\n",
    "                temp1 = sentence.split() # \\s 기준으로 분리\n",
    "                if len(temp1)==0: continue # 가끔 비어있는 애들이 있음\n",
    "                assert type(temp1[0])==str, print(f'>>>>여기여기 {idx}')\n",
    "                temp.append(temp1)\n",
    "            \n",
    "            temp2 = flatten(temp)\n",
    "            assert type(temp2[0]) == str\n",
    "\n",
    "            if N==1:    \n",
    "                basic.update(temp2)\n",
    "            \n",
    "            elif N==2:\n",
    "                temp3 = list(zip(temp2, temp2[1:]))\n",
    "                basic.update(temp3)\n",
    "            elif N==3: \n",
    "                temp3 = list(zip(temp2, temp2[1:], temp2[2:]))\n",
    "                basic.update(temp3)\n",
    "\n",
    "        return basic\n",
    "\n",
    "    def entropy(count_dict, sample=False): # 계산 샘플 출력할 수 있게 추가하기\n",
    "        val = 0 ; total_cnt = sum(count_dict.values())\n",
    "        \n",
    "        for value in count_dict.values():\n",
    "            prob = value/total_cnt\n",
    "            vl_log = np.log2(prob) # base 2\n",
    "            val -= prob*vl_log\n",
    "\n",
    "        # if sample == True: print(window_not_in_training, '\\n', support_set)\n",
    "        return val\n",
    "\n",
    "    def cross_entropy(true_d, model_d, sample=False):\n",
    "        val = 0 \n",
    "        window_not_in_training = [window  for window in model_d.keys() if window not in list(true_d.keys())]\n",
    "        window_not_in_training_dict = {i: 1 for i in window_not_in_training}\n",
    "        \n",
    "        support_set_ = Counter(true_d)\n",
    "        support_set = {k: v+1 for k, v in support_set_.items()}\n",
    "        support_set.update(window_not_in_training_dict)\n",
    "        total_true = sum(support_set.values()) ; total_model = sum(model_d.values()) \n",
    "        \n",
    "        for key, model_value in model_d.items():\n",
    "            prob_true = support_set[key]/total_true\n",
    "            prob_model = model_value/total_model\n",
    "            # log_model = np.log2(prob_model)\n",
    "            log_true = np.log2(prob_true)\n",
    "            \n",
    "            val -= prob_model * log_true\n",
    "        \n",
    "        if sample == True: print(window_not_in_training, '\\n', support_set)\n",
    "        return val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>> 자모 Counter({('ㅏ', ' '): 2, ('ㄴ', 'ㅏ'): 1, (' ', 'ㄴ'): 1, ('ㄴ', 'ㅡ'): 1, ('ㅡ', 'ㄴ'): 1, ('ㄴ', None): 1, (None, 'ㄱ'): 1, ('ㄱ', 'ㅘ'): 1, ('ㅘ', ' '): 1, (' ', 'ㅈ'): 1, ('ㅈ', 'ㅔ'): 1, ('ㅔ', ' '): 1, (' ', 'ㄹ'): 1, ('ㄹ', 'ㅡ'): 1, ('ㅡ', 'ㄹ'): 1, ('ㄹ', None): 1, (None, 'ㅈ'): 1, ('ㅈ', 'ㅏ'): 1, ('ㅏ', 'ㄹ'): 1, ('ㄹ', 'ㅎ'): 1, ('ㅎ', 'ㅐ'): 1, ('ㅐ', 'ㅆ'): 1, ('ㅆ', 'ㄷ'): 1, ('ㄷ', 'ㅏ'): 1}) \n",
      " >>>>>>>>>>>>>>> 글자 Counter({('나', '는'): 1, ('는', ' '): 1, (' ', '과'): 1, ('과', '제'): 1, ('제', '를'): 1, ('를', ' '): 1, (' ', '잘'): 1, ('잘', '했'): 1, ('했', '다'): 1}) \n",
      " >>>>>>>>>>>>>>> 어절 Counter({('나는', '과제를'): 1, ('과제를', '잘했다'): 1}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ngram 테스트\n",
    "train = ['나는 과제를 잘했다']\n",
    "print(\n",
    "'>>>>>>>>>>>>>>> 자모',\n",
    "ngram_huni.fit((train), N=2, split_type='자모'), '\\n',\n",
    "'>>>>>>>>>>>>>>> 글자',\n",
    "ngram_huni.fit((train), N=2, split_type='글자'), '\\n',\n",
    "'>>>>>>>>>>>>>>> 어절',\n",
    "ngram_huni.fit((train), N=2, split_type='어절'), '\\n', \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.74917466839001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 엔트로피 테스트\n",
    "training = {'나는': 10, '과제를': 2, '잘': 3, '했다': 7}\n",
    "ngram_huni.entropy(training, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['힝', '그거그거'] \n",
      " {'나는': 11, '과제를': 3, '잘': 4, '했다': 8, '힝': 1, '그거그거': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.807354922057605"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 교차 엔트로피 테스트\n",
    "training = {'나는': 10, '과제를': 2, '잘': 3, '했다': 7}\n",
    "test = {'힝': 2, '그거그거':1}\n",
    "ngram_huni.cross_entropy(training, test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:00<00:00, 256878.00it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>> 자모 분리 시작\n",
      "NIRW1900000011.json 자모 1 >>> 코퍼스 규모: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:08<00:17,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIRW1900000011.json 자모 2 >>> 코퍼스 규모: 782\n",
      ">>>>>>>> 글자 분리 시작\n",
      "NIRW1900000011.json 글자 1 >>> 코퍼스 규모: 1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:10<00:06,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIRW1900000011.json 글자 2 >>> 코퍼스 규모: 72296\n",
      ">>>>>>>> 어절 분리 시작\n",
      "NIRW1900000011.json 어절 1 >>> 코퍼스 규모: 81221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.89s/it]\n",
      "100%|██████████| 406/406 [00:00<00:00, 328172.56it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIRW1900000011.json 어절 2 >>> 코퍼스 규모: 277289\n",
      ">>>>>>>> 자모 분리 시작\n",
      "NWRW1800000045.json 자모 1 >>> 코퍼스 규모: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:02<00:05,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWRW1800000045.json 자모 2 >>> 코퍼스 규모: 772\n",
      ">>>>>>>> 글자 분리 시작\n",
      "NWRW1800000045.json 글자 1 >>> 코퍼스 규모: 1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:03<00:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWRW1800000045.json 글자 2 >>> 코퍼스 규모: 53910\n",
      ">>>>>>>> 어절 분리 시작\n",
      "NWRW1800000045.json 어절 1 >>> 코퍼스 규모: 46341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n",
      "100%|██████████| 100/100 [00:00<00:00, 69626.56it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWRW1800000045.json 어절 2 >>> 코퍼스 규모: 105067\n",
      ">>>>>>>> 자모 분리 시작\n",
      "EBRW1908000138.json 자모 1 >>> 코퍼스 규모: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:01<00:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBRW1908000138.json 자모 2 >>> 코퍼스 규모: 719\n",
      ">>>>>>>> 글자 분리 시작\n",
      "EBRW1908000138.json 글자 1 >>> 코퍼스 규모: 1128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBRW1908000138.json 글자 2 >>> 코퍼스 규모: 25318\n",
      ">>>>>>>> 어절 분리 시작\n",
      "EBRW1908000138.json 어절 1 >>> 코퍼스 규모: 19208\n",
      "EBRW1908000138.json 어절 2 >>> 코퍼스 규모: 46804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>데이터셋</th>\n",
       "      <th>분리유형</th>\n",
       "      <th>N-gram</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>KL_divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NIRW1900000011.json</td>\n",
       "      <td>자모</td>\n",
       "      <td>1</td>\n",
       "      <td>4.375139</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIRW1900000011.json</td>\n",
       "      <td>자모</td>\n",
       "      <td>2</td>\n",
       "      <td>7.495124</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIRW1900000011.json</td>\n",
       "      <td>글자</td>\n",
       "      <td>1</td>\n",
       "      <td>7.954627</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NIRW1900000011.json</td>\n",
       "      <td>글자</td>\n",
       "      <td>2</td>\n",
       "      <td>13.464417</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NIRW1900000011.json</td>\n",
       "      <td>어절</td>\n",
       "      <td>1</td>\n",
       "      <td>13.872882</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NIRW1900000011.json</td>\n",
       "      <td>어절</td>\n",
       "      <td>2</td>\n",
       "      <td>17.754399</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NWRW1800000045.json</td>\n",
       "      <td>자모</td>\n",
       "      <td>1</td>\n",
       "      <td>4.373531</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NWRW1800000045.json</td>\n",
       "      <td>자모</td>\n",
       "      <td>2</td>\n",
       "      <td>7.519986</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NWRW1800000045.json</td>\n",
       "      <td>글자</td>\n",
       "      <td>1</td>\n",
       "      <td>8.077953</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NWRW1800000045.json</td>\n",
       "      <td>글자</td>\n",
       "      <td>2</td>\n",
       "      <td>13.904116</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NWRW1800000045.json</td>\n",
       "      <td>어절</td>\n",
       "      <td>1</td>\n",
       "      <td>14.063075</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NWRW1800000045.json</td>\n",
       "      <td>어절</td>\n",
       "      <td>2</td>\n",
       "      <td>16.554635</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EBRW1908000138.json</td>\n",
       "      <td>자모</td>\n",
       "      <td>1</td>\n",
       "      <td>4.308668</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EBRW1908000138.json</td>\n",
       "      <td>자모</td>\n",
       "      <td>2</td>\n",
       "      <td>7.382353</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EBRW1908000138.json</td>\n",
       "      <td>글자</td>\n",
       "      <td>1</td>\n",
       "      <td>7.830959</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EBRW1908000138.json</td>\n",
       "      <td>글자</td>\n",
       "      <td>2</td>\n",
       "      <td>12.668219</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EBRW1908000138.json</td>\n",
       "      <td>어절</td>\n",
       "      <td>1</td>\n",
       "      <td>12.735195</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EBRW1908000138.json</td>\n",
       "      <td>어절</td>\n",
       "      <td>2</td>\n",
       "      <td>15.30497</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   데이터셋 분리유형 N-gram    Entropy Cross Entropy KL_divergence\n",
       "0   NIRW1900000011.json   자모      1   4.375139             X             X\n",
       "1   NIRW1900000011.json   자모      2   7.495124             X             X\n",
       "2   NIRW1900000011.json   글자      1   7.954627             X             X\n",
       "3   NIRW1900000011.json   글자      2  13.464417             X             X\n",
       "4   NIRW1900000011.json   어절      1  13.872882             X             X\n",
       "5   NIRW1900000011.json   어절      2  17.754399             X             X\n",
       "6   NWRW1800000045.json   자모      1   4.373531             X             X\n",
       "7   NWRW1800000045.json   자모      2   7.519986             X             X\n",
       "8   NWRW1800000045.json   글자      1   8.077953             X             X\n",
       "9   NWRW1800000045.json   글자      2  13.904116             X             X\n",
       "10  NWRW1800000045.json   어절      1  14.063075             X             X\n",
       "11  NWRW1800000045.json   어절      2  16.554635             X             X\n",
       "12  EBRW1908000138.json   자모      1   4.308668             X             X\n",
       "13  EBRW1908000138.json   자모      2   7.382353             X             X\n",
       "14  EBRW1908000138.json   글자      1   7.830959             X             X\n",
       "15  EBRW1908000138.json   글자      2  12.668219             X             X\n",
       "16  EBRW1908000138.json   어절      1  12.735195             X             X\n",
       "17  EBRW1908000138.json   어절      2   15.30497             X             X"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP = RE_huniv2()\n",
    "set_ls = [FILE_NAME, test_NAME, test2_NAME] ; split_ls = '자모 글자 어절'.split()\n",
    "df = pd.DataFrame(columns=['데이터셋', '분리유형', 'N-gram', 'Entropy', 'Cross Entropy', 'KL_divergence'],\n",
    "                                                    index = range(27))\n",
    "\n",
    "training = {}\n",
    "for data_set in set_ls:\n",
    "    loaded = load_data(data_set)\n",
    "    \n",
    "    for splt in tqdm(split_ls): # 자모, 글자 분리는 어절 분리와 다르게 처리해야함\n",
    "        print(f'>>>>>>>> {splt} 분리 시작')\n",
    "        if splt != '어절':\n",
    "            cleaned = NLP.list_cleaning(loaded, type=1)\n",
    "        \n",
    "        elif splt == '어절':\n",
    "            cleaned = NLP.list_cleaning(loaded, type=2)\n",
    "        \n",
    "        for ngram in range(1,3):\n",
    "            rs1 = ngram_huni.fit(cleaned, N=ngram, split_type=splt)\n",
    "            if data_set == FILE_NAME: training[splt] = rs1 # 훈련자료의 경우 저장함\n",
    "    \n",
    "            val = ngram_huni.entropy(rs1)\n",
    "            print(data_set, splt, ngram, f'>>> 코퍼스 규모: {len(rs1.keys())}')\n",
    "            \n",
    "            if data_set == FILE_NAME: cros_val = val\n",
    "            else: cros_val = ngram_huni.cross_entropy(training[splt], rs1)\n",
    "            temp = pd.DataFrame([data_set, splt, ngram, val, cros_val, cros_val - val]).T\n",
    "            temp = pd.DataFrame([data_set, splt, ngram, val, 'X', 'X']).T\n",
    "            temp.columns = ['데이터셋', '분리유형', 'N-gram', 'Entropy', 'Cross Entropy', 'KL_divergence']\n",
    "            df = pd.concat([df, temp], axis=0)\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e7cc6c9e0688f13882f545a2e978df997152a3234a397a2d0b8069758700627"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
