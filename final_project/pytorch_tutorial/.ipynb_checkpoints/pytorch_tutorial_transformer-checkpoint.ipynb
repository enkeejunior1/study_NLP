{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1c9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392585e8",
   "metadata": {},
   "source": [
    "Transformer 의 구조는 다음과 같다.\n",
    "\n",
    "1. Encoder + Decoder\n",
    "2. Encoder = Enc_sublayer + Enc_sublayer + Enc_sublayer + Enc_sublayer + Enc_sublayer + Enc_sublayer\n",
    "3. Enc_sublayer = residual connection [Multi-head attention + Feed Forward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4073a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, input_sentence, some_sentence, mask):\n",
    "        context         = self.encoder(input_sentence, mask)\n",
    "        output_sentence = self.decoder(some_sentence, context)\n",
    "        return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e51691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, n_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(copy.deepcopy(encoder_layer))\n",
    "            \n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91fc57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer, norm_layer):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention_layer = multi_head_attention_layer\n",
    "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
    "        self.residual_connection_layers = [ResidualConnectionLayer(copy.deepcopy(norm_layer)) for i in range(2)] #!#\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        out = self.residual_connection_layers[0](x, lambda x : self.multi_head_attention_layer(query = x, key = x, value = x, mask = mask))\n",
    "        out = self.residual_connection_layers[1](x, lambda x : self.position_wise_feed_forward_layer(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b470ac",
   "metadata": {},
   "source": [
    "### self-attention\n",
    "self-Attention 에서의 계산하는 대상은 Query 가 주어졌을 때, 다른 token 에 대한 관계다. 먼저 Query, Key, Value 3가지 input 을 받게 된다.\n",
    "\n",
    "1. Query: 현재 시점의 token을 의미\n",
    "2. Key: attention을 구하고자 하는 대상 token을 의미\n",
    "3. Value: attention을 구하고자 하는 대상 token을 의미 (Key와 동일한 token)\n",
    "\n",
    "Query, Key, Value 는 input으로 들어오는 token embedding vector를 fully connected layer에 넣어 세 vector를 만들어낸다. ([n, d_embed] -> [n, dk])\n",
    "\n",
    "수식은 이하와 같이 계산된다. \n",
    "\n",
    "$$\n",
    "\\text { Query's Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\n",
    "$$\n",
    "\n",
    "Q[1, dk] x K.T[dk, n] = Attention Score [1, n]\n",
    "즉, Attention score 는 Query 가 각각의 key 와의 유사도를 나타내다. (내적 = 유사도)\n",
    "\n",
    "이렇게 구한 attention score 를 value 에 곱해주면 attention 결과를 계산할 수 있다. \n",
    "\n",
    "AS[1, n] x V[n, dk] = Query's attention [1, dk]\n",
    "\n",
    "1개의 token 을 n개로 확장하려면 1 을 n 으로 바꿔주기만 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb472b",
   "metadata": {},
   "source": [
    "실제 논문에서는 multi-head attention 모델이 사용되는데, self-attention 을 병렬적으로 여러개 수행하는 것이다. 이를 통해 덜 중요한 attention 까지 포함할 수 있는 attention 을 얻을 수 있도록 돕는다.\n",
    "\n",
    "h = 8 이라면, query, key, value's shape: (n_batch, seq_len, d_k * h) 가 된다. 이제 d_k * h 를 d_model 이라고 부른다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3228dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, qkv_fc_layer, fc_layer):\n",
    "        # b : batch, n : seq_len, h, d_k : key dim\n",
    "        # qkv_fc_layer = (d_embed, d_model)\n",
    "        # fc_layer = (d_model, d_embed)\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.query_fc_layer = copy.deepcopy(qkv_fc_layer) # b n h d_embedding -> b n h d_k\n",
    "        self.key_fc_layer   = copy.deepcopy(qkv_fc_layer)\n",
    "        self.value_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.fc_layer = fc_layer\n",
    "        \n",
    "    def calculate_attention(self, query, key, value, mask):\n",
    "        #!# query, key, value's shape: (n_batch, seq_len, d_k)\n",
    "        d_k = key.size(-1)\n",
    "        attention_score = torch.einsum('bik, bjk -> bij', query, key) / torch.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_score.masked_fill_(mask == 0, -1e9)\n",
    "            # attention_score = attention_score.masked_fill(mask == 0, -1e9) #!# almost pseudo-code..\n",
    "        attention_prob = F.softmax(attention_score, dim = -1)\n",
    "        return torch.einsum('bik, bkj -> bij', attention_prob, value)\n",
    "      \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # query, key, value's shape: (n_batch, seq_len, d_embed)\n",
    "        # mask's shape: (n_batch, seq_len, seq_len)\n",
    "        n_batch = query.shape[0]\n",
    "        \n",
    "        def transform(x, fc_layer): \n",
    "            # x   : (n_batch, seq_len, d_embed)\n",
    "            # out : (n_batch, h, seq_len, d_k)\n",
    "            out = fc_layer(x) # (n_batch, seq_len, h * d_k)\n",
    "            out = rearrange(out, 'b n (h d_k) -> b h n d_k', h = self.h) #!# need to check\n",
    "            return out\n",
    "        \n",
    "        query = transform(query, self.query_fc_layer)\n",
    "        key   = transform(key,   self.key_fc_layer)\n",
    "        value = transform(value, self.value_fc_layer)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = rearrange(mask, 'b n m -> b () n m') # m is also seq_len\n",
    "        \n",
    "        out = self.calculate_attention(query, key, value, mask) # n_batch, h, seq_len, d_k -> n_batch, h, seq_len, d_k\n",
    "        out = rearrange(out, 'b h n d_k -> b n (h d_k)') # n_batch, seq_len, d_model\n",
    "        out = self.fc_layer(out) # n_batch, seq_len, d_model -> n_batch, seq_len, d_embed\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffef7c",
   "metadata": {},
   "source": [
    "### Position-wise Feed Forward Layer\n",
    "\n",
    "Attetion 의 결과를 ADD & Norm 을 넣어 처리하고 Feed Forward layer 에 넣어준다. 이때, 사용하는 layer 가 바로 position-wise feed forward layer 다.\n",
    "\n",
    "$$\n",
    "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bda3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForwardLayer(nn.Module):\n",
    "    def __init__(self, first_fc_layer, second_fc_layer):\n",
    "        self.first_fc_layer  = first_fc_layer\n",
    "        self.second_fc_layer = second_fc_layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        fc_layer = nn.sequential(\n",
    "            self.first_fc_layer(),\n",
    "            F.relu(),\n",
    "            self.dropout(),\n",
    "            self.second_fc_layer()\n",
    "        )\n",
    "        out = fc_layer(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d969a54e",
   "metadata": {},
   "source": [
    "### residual connection + Layer Normalization\n",
    "\n",
    "instead of using $y = f(x)$, choose $y = f(x) + x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cf43c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnectionLayer(nn.Module):\n",
    "    def __init__(self, norm_layer):\n",
    "        super(ResidualConnectionLayer, self).__init__()\n",
    "        self.norm_layer = norm_layer\n",
    "        \n",
    "    def forward(self, x, sub_layer):\n",
    "        out = sub_layer(x) + x\n",
    "        out = self.norm_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ecfd8",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "먼저 teacher force 방식으로 학습을 시키기 때문에 masking 을 위한 함수가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13598b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attention_shape = (1, size, size)\n",
    "    mask = torch.triu(torch.ones(attention_shape), k = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
