{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "328fec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9008dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, input_sentence, some_sentence):\n",
    "        context         = self.encoder(input_sentence)\n",
    "        output_sentence = self.decoder(some_sentence, context)\n",
    "        return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b7c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, n_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(copy.deepcopy(encoder_layer))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7efafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, multi_head_attention_layer, position_wise_feed_forward_layer):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention_layer = multi_head_attention_layer\n",
    "        self.position_wise_feed_forward_layer = position_wise_feed_forward_layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.multi_head_attention_layer(x)\n",
    "        out = self.position_wise_feed_forward_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f828b97",
   "metadata": {},
   "source": [
    "### self-attention\n",
    "self-Attention 에서의 계산하는 대상은 Query 가 주어졌을 때, 다른 token 에 대한 관계다. 먼저 Query, Key, Value 3가지 input 을 받게 된다.\n",
    "\n",
    "1. Query: 현재 시점의 token을 의미\n",
    "2. Key: attention을 구하고자 하는 대상 token을 의미\n",
    "3. Value: attention을 구하고자 하는 대상 token을 의미 (Key와 동일한 token)\n",
    "\n",
    "Query, Key, Value 는 input으로 들어오는 token embedding vector를 fully connected layer에 넣어 세 vector를 만들어낸다. ([n, d_embed] -> [n, dk])\n",
    "\n",
    "수식은 이하와 같이 계산된다. \n",
    "\n",
    "$$\n",
    "\\text { Query's Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\n",
    "$$\n",
    "\n",
    "Q[1, dk] x K.T[dk, n] = Attention Score [1, n]\n",
    "즉, Attention score 는 Query 가 각각의 key 와의 유사도를 나타내다. (내적 = 유사도)\n",
    "\n",
    "이렇게 구한 attention score 를 value 에 곱해주면 attention 결과를 계산할 수 있다. \n",
    "\n",
    "AS[1, n] x V[n, dk] = Query's attention [1, dk]\n",
    "\n",
    "1개의 token 을 n개로 확장하려면 1 을 n 으로 바꿔주기만 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239cb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_attention(self, query, key, value, mask):\n",
    "    # query, key, value's shape: (n_batch, seq_len, d_k)\n",
    "    d_k = key.size(-1)\n",
    "    attention_score = torch.einsum('bik, bjk -> bij', query, key) / torch.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attention_score.masked_fill_(mask == 0, -1e9)\n",
    "        # attention_score = attention_score.masked_fill(mask == 0, -1e9) #!# almost pseudo-code..\n",
    "    attention_prob = F.softmax(attention_score, dim = -1)\n",
    "    return torch.einsum('bik, bkj -> bij', attention_prob, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31350932",
   "metadata": {},
   "source": [
    "실제 논문에서는 multi-head attention 모델이 사용되는데, self-attention 을 병렬적으로 여러개 수행하는 것이다. 이를 통해 덜 중요한 attention 까지 포함할 수 있는 attention 을 얻을 수 있도록 돕는다.\n",
    "\n",
    "h = 8 이라면, query, key, value's shape: (n_batch, seq_len, d_k * h) 가 된다. 이제 d_k * h 를 d_model 이라고 부른다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3817ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, qkv_fc_layer, fc_layer):\n",
    "        # qkv_fc_layer's shape: (n_batch, d_embed, d_model)\n",
    "        # fc_layer's shape: (n_batch, d_model, d_embed)\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.query_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.key_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.value_fc_layer = copy.deepcopy(qkv_fc_layer)\n",
    "        self.fc_layer = fc_layer\n",
    "      \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # query, key, value's shape: (n_batch, seq_len, d_embed)\n",
    "        # mask's shape: (n_batch, seq_len, seq_len)\n",
    "        n_batch = query.shape[0]\n",
    "        \n",
    "        def transform(x, fc_layer): \n",
    "            # x : (n_batch, seq_len, d_model)\n",
    "            out = fc_layer(x)\n",
    "            out = out.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
